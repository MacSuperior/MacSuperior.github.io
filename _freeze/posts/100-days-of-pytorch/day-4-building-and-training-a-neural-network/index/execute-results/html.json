{
  "hash": "b27c83bbe76d6b48ae5dc0c8647f9371",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Day 4 - PyTorch Basics: Building and Training a Neural Network\"\npagetitle: \"PyTorch Basics: Building and Training a Neural Network\"\n\ndescription: >\n  Learn how to use `torch.nn` to build the neural network structure, and how to train the model to recognize images, using a training and evaluation loop. It assumes you are already familiar with the theory behind neural networks (i.e loss functions, gradient descent).\n\ndescription-meta: >\n  Learn how to use `torch.nn` to build the neural network structure, and how to train the model to recognize images, using a training and evaluation loop.\n\ndate: \"2025-05-11\"\ndate-modified: \"2025-05-12\"\ndate-meta: \"2025-05-12\"\n\nkeywords: [\"Neural Network\"]\njupyter: python3\n---\n\n## Defining the Network \n\n<details>\n<summary>Show imports</summary>\n\n::: {#f10171ff .cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n```\n:::\n\n\n</details>\n\n\nPyTorch allows us to create any neural network, using predefined building blocks from the module `torch.nn`. Every neural network we create should specify the `forward` method\n\n::: {#4e1f9f5f .cell execution_count=2}\n``` {.python .cell-code}\nclass OurNeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Define neural net structure here, so we can store weights in them.\n        self.flatten = nn.Flatten()\n        self.linear_relu_chain = nn.Sequential(\n            nn.Linear(in_features=28*28, out_features=512),\n            nn.ReLU(),\n            nn.Linear(in_features=512, out_features=512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n    \n    def forward(self, input):\n        # Use neural net structure to pass input data through\n\n        input = self.flatten(input) # Shape: (28,28) -> shape: (784)\n\n        predictions = self.linear_relu_chain(input) # Shape: (784) -> shape: (512) -> shape: (512) -> shape: (10)\n        \n        return predictions\n```\n:::\n\n\nLets instantiate it. In PyTorch, we also have to specify it on what device type we want to train our model. This allows for quicker training, depending on device type.\n\n::: {#29630da3 .cell execution_count=3}\n``` {.python .cell-code}\nmodel = OurNeuralNetwork().to(\"cpu\") # or cuda, mps, mtia, xpu\nprint(\"using cpu\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nusing cpu\n```\n:::\n:::\n\n\n::: {#bd52b7cf .cell execution_count=4}\n``` {.python .cell-code}\nmodel\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nOurNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_chain): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n```\n:::\n:::\n\n\n## Testing the Network\n\nLets simulate an 28x28 pixel image with some random numbers\n\n::: {#56f63179 .cell execution_count=5}\n``` {.python .cell-code}\nshape = (1, 28, 28)\nfake_image = torch.rand(shape, device=\"cpu\") # the tensor needs to be on the same device as the model\n```\n:::\n\n\n::: {#ab0e90aa .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show plotting code\"}\nimport matplotlib.pyplot as plt\n\nplt.imshow(fake_image.squeeze())\nplt.axis(\"off\")\nplt.title(\"Fake number - random 28x28 tensor\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=389 height=409}\n:::\n:::\n\n\n::: {#438b772f .cell execution_count=7}\n``` {.python .cell-code}\nmodel_outputs = model(fake_image)\n```\n:::\n\n\nEven though we specified `nn.Linear(in_features=28*28)`, we set `shape = (1, 28, 28)` and not to `(28, 28)`. This is because the model expects the first number to be the amount of images we use per batch of training. Since we are not actually training the model right now we set it to `1`.\n\n::: {#57f72a47 .cell execution_count=8}\n``` {.python .cell-code}\nprobabilities = nn.Softmax(dim=1)(model_outputs)\npredicted_label = probabilities.argmax(1)\npredicted_label.item()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n3\n```\n:::\n:::\n\n\nOf course, the output is completely random, since the network was not trained and since the image was not actually a number. \n\n## Training and Evaluating the Network\n\nLet's train the network on the FashionMNIST dataset to classify images. For this we import the dataset using the code from a previous post explaining PyTorch [Datasets and DataLoaders](../datasets-dataloaders/index.qmd)\n\n::: {#3bccd11f .cell execution_count=9}\n``` {.python .cell-code code-fold=\"false\" code-summary=\"Show code\"}\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=transforms.ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=transforms.ToTensor()\n)\n\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n```\n:::\n\n\nWe need to specify 3 parameters:\n\n- **Number of Epochs -** How many times to iterate over the entire dataset.\n\n- **Learning Rate -** A scaling factor specifying how much to update the model parameters each iteration\n\n- **Batch Size -** How much examples to iterate over before updating parameters.\n\n\nPer batch of images, model parameters are updated, until all images in the dataset have been seen. This process is repeated for the specified number of epochs.\n\n::: {#a0dfd253 .cell execution_count=10}\n``` {.python .cell-code}\nlearning_rate = 1e-3\nbatch_size = 64\nepochs = 5\n```\n:::\n\n\nIn each epoch we also want to check the model performance (to see if it is improving). That is why we will build two loops. \n\n- **Training Loop -** Update parameters by showing model images with labels (from `train_dataloader`)\n\n- **Checking Loop -** Evaluate model performance with learned parameters from the training loop on new images (from `test_dataloader`)\n\n::: {#fec94579 .cell execution_count=11}\n``` {.python .cell-code}\ndef train_loop(dataloader, model, loss_func, optimizer):\n    size = len(dataloader.dataset)\n    model.train() # Set model to training mode\n\n    # Update parameters each new batch\n    for batch, (images, labels) in enumerate(dataloader):\n        model_predictions = model(images)\n        loss = loss_func(model_predictions, labels)\n\n        # Compute gradient with backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        # Something to look at while model trains\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * batch_size + len(images)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n\ndef checking_loop(dataloader, model, loss_func):\n    size = len(dataloader.dataset)\n    number_of_batches = len(dataloader)\n    test_loss, correct_amount = 0, 0\n\n    model.eval() # Set model to check/test mode\n\n    with torch.no_grad(): # We don't need to update parameters anymore. This speeds up testing.\n\n        # This dataloader contains the test images\n        for images, labels in dataloader:\n            model_predictions = model(images)\n            \n            loss = loss_func(model_predictions, labels).item()\n            test_loss += loss\n\n            predicted_labels = nn.Softmax(dim=1)(model_predictions).argmax(1)\n            correct = predicted_labels == labels\n            # Turn every 'True' into a 1, and sum over them, converting the resulting tensor to a python integer\n            correct_amount += correct.type(torch.float).sum().item()\n\n    test_loss /= number_of_batches\n    correct_amount /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct_amount):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n```\n:::\n\n\nLets train our model, passing a loss function and optimizer.\n\n::: {#203c3864 .cell execution_count=12}\n``` {.python .cell-code}\nloss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n```\n:::\n\n\n::: {#122845b6 .cell execution_count=13}\n``` {.python .cell-code}\n# Train the model\nepochs = 10\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train_dataloader, model, loss_function, optimizer)\n    checking_loop(test_dataloader, model, loss_function)\nprint(\"Done!\")\n```\n:::\n\n\n<details>\n<summary>Show output</summary>\n\n::: {#9a4d4457 .cell execution_count=14}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nEpoch 1\n-------------------------------\nloss: 1.469673  [   64/60000]\nloss: 1.433510  [ 6464/60000]\nloss: 1.416146  [12864/60000]\nloss: 1.377274  [19264/60000]\nloss: 1.353796  [25664/60000]\nloss: 1.429164  [32064/60000]\nloss: 1.270087  [38464/60000]\nloss: 1.251621  [44864/60000]\nloss: 1.361366  [51264/60000]\nloss: 1.273360  [57664/60000]\nTest Error: \n Accuracy: 63.7%, Avg loss: 1.244470 \n\nEpoch 2\n-------------------------------\nloss: 1.250782  [   64/60000]\nloss: 1.358424  [ 6464/60000]\nloss: 1.121522  [12864/60000]\nloss: 1.036822  [19264/60000]\nloss: 1.188575  [25664/60000]\nloss: 1.127103  [32064/60000]\nloss: 1.138114  [38464/60000]\nloss: 1.059073  [44864/60000]\nloss: 1.033557  [51264/60000]\nloss: 1.098557  [57664/60000]\nTest Error: \n Accuracy: 65.0%, Avg loss: 1.083130 \n\nEpoch 3\n-------------------------------\nloss: 1.070835  [   64/60000]\nloss: 1.022191  [ 6464/60000]\nloss: 1.005594  [12864/60000]\nloss: 0.993012  [19264/60000]\nloss: 1.047417  [25664/60000]\nloss: 1.001495  [32064/60000]\nloss: 1.095251  [38464/60000]\nloss: 0.926997  [44864/60000]\nloss: 0.960782  [51264/60000]\nloss: 0.937367  [57664/60000]\nTest Error: \n Accuracy: 66.1%, Avg loss: 0.979457 \n\nEpoch 4\n-------------------------------\nloss: 0.900051  [   64/60000]\nloss: 1.099103  [ 6464/60000]\nloss: 1.052053  [12864/60000]\nloss: 0.843110  [19264/60000]\nloss: 0.914962  [25664/60000]\nloss: 1.017330  [32064/60000]\nloss: 0.707650  [38464/60000]\nloss: 0.890666  [44864/60000]\nloss: 1.078490  [51264/60000]\nloss: 0.758047  [57664/60000]\nTest Error: \n Accuracy: 67.3%, Avg loss: 0.909492 \n\nEpoch 5\n-------------------------------\nloss: 0.935071  [   64/60000]\nloss: 0.930360  [ 6464/60000]\nloss: 0.886458  [12864/60000]\nloss: 0.747989  [19264/60000]\nloss: 0.919060  [25664/60000]\nloss: 0.857149  [32064/60000]\nloss: 0.808115  [38464/60000]\nloss: 0.957309  [44864/60000]\nloss: 0.915866  [51264/60000]\nloss: 1.035016  [57664/60000]\nTest Error: \n Accuracy: 68.0%, Avg loss: 0.857042 \n\nEpoch 6\n-------------------------------\nloss: 0.711309  [   64/60000]\nloss: 0.731404  [ 6464/60000]\nloss: 0.778495  [12864/60000]\nloss: 0.826608  [19264/60000]\nloss: 0.690381  [25664/60000]\nloss: 0.793883  [32064/60000]\nloss: 1.049005  [38464/60000]\nloss: 0.860935  [44864/60000]\nloss: 0.850578  [51264/60000]\nloss: 0.894870  [57664/60000]\nTest Error: \n Accuracy: 69.2%, Avg loss: 0.820537 \n\nEpoch 7\n-------------------------------\nloss: 0.751307  [   64/60000]\nloss: 0.690765  [ 6464/60000]\nloss: 0.885832  [12864/60000]\nloss: 0.810388  [19264/60000]\nloss: 0.656271  [25664/60000]\nloss: 0.795354  [32064/60000]\nloss: 0.873639  [38464/60000]\nloss: 0.952544  [44864/60000]\nloss: 0.621379  [51264/60000]\nloss: 0.782824  [57664/60000]\nTest Error: \n Accuracy: 70.0%, Avg loss: 0.791091 \n\nEpoch 8\n-------------------------------\nloss: 0.706885  [   64/60000]\nloss: 0.791194  [ 6464/60000]\nloss: 0.665691  [12864/60000]\nloss: 0.586563  [19264/60000]\nloss: 0.746921  [25664/60000]\nloss: 0.670890  [32064/60000]\nloss: 0.818113  [38464/60000]\nloss: 0.725863  [44864/60000]\nloss: 0.793836  [51264/60000]\nloss: 0.689501  [57664/60000]\nTest Error: \n Accuracy: 71.0%, Avg loss: 0.764419 \n\nEpoch 9\n-------------------------------\nloss: 0.579552  [   64/60000]\nloss: 0.783948  [ 6464/60000]\nloss: 0.766569  [12864/60000]\nloss: 0.831361  [19264/60000]\nloss: 0.964704  [25664/60000]\nloss: 0.772870  [32064/60000]\nloss: 0.836838  [38464/60000]\nloss: 0.806005  [44864/60000]\nloss: 0.795276  [51264/60000]\nloss: 0.934505  [57664/60000]\nTest Error: \n Accuracy: 72.8%, Avg loss: 0.744880 \n\nEpoch 10\n-------------------------------\nloss: 0.493929  [   64/60000]\nloss: 0.790016  [ 6464/60000]\nloss: 0.750857  [12864/60000]\nloss: 0.762535  [19264/60000]\nloss: 0.822756  [25664/60000]\nloss: 0.723158  [32064/60000]\nloss: 0.535035  [38464/60000]\nloss: 0.708430  [44864/60000]\nloss: 0.695287  [51264/60000]\nloss: 0.616080  [57664/60000]\nTest Error: \n Accuracy: 73.5%, Avg loss: 0.724422 \n\nDone!\n```\n:::\n:::\n\n\n</details>\n\n## Further reading\n\n- [torch.nn under the hood](https://docs.pytorch.org/tutorials/beginner/nn_tutorial.html)\n\n- [torch.nn examples](https://docs.pytorch.org/tutorials/beginner/pytorch_with_examples.html#nn-module)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}