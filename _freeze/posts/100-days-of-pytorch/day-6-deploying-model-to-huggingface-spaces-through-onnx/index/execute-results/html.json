{
  "hash": "8dcc69964e32f1979c0ccc69f37770a9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Day 6: PyTorch Basics - Deploying Model to HuggingFace Spaces With Gradio and ONNX\"\npagetitle: \"PyTorch Basics - Deploying Model to HuggingFace Spaces Through Gradio and ONNX\"\ndescription: \"Deploying your model might seem like a daunting task, and in some cases, it certainly is. But it is arguably one of the most import things to learn, since a model that never makes it out of your local machine is not useful to anybody. In this post we learn how to convert a trained model to the ONNX format, and deploy it permanently on HuggingFace for free with Gradio.\"\ndescription-meta: \"Learn how to convert a trained model to the ONNX format, and deploy it permanently on HuggingFace for free with Gradio.\"\nimage: \"thumbnail.png\"\ndate: \"2025-05-13\"\ndate-meta: \"2025-05-13\"\n\nkeywords: [ONNX, HuggingFace, HuggingFace Spaces, Deploy Model, Gradio]\njupyter: python3\n---\n\nAt the end of this tutorial, we will have a running deployment of an ONNX model on Hugging Face Spaces:\n<iframe\n  src=\"https://liamgroen-onnx-demo.hf.space/\"\n  frameborder=\"0\"\n  width=\"100%\"\n  height=\"600px\"\n  style=\"min-width:400px;\"\n></iframe>\n\n## What is ONNX?\n\nThere exist many different deep learning frameworks, across many different programming language. [ONNX](https://onnx.ai/) is a standard that defines a common set of building blocks and file format so that no matter what technology is used to train a model, when it is rendered to ONNX, it can be deployed [virtually anywhere](https://onnx.ai/supported-tools.html#deployModel).\n\n## Converting to ONNX\nIn this section I assume that you have a trained model called `torch_model`. If you don't have one, or don't know how to train your own model yet, [this post](../day-4-building-and-training-a-neural-network/index.qmd) explains how to build and train your own model.\n\nLet's export our model to ONNX format. Since `onnx.export` runs the model, we need to supply an example input. Furthermore If we don't want the batch size to be stationary, we need to set the `dynamic_axes` parameter.\n\n\n\n::: {#ae2ed722 .cell execution_count=2}\n``` {.python .cell-code}\nbatch_size = 1 # Random batch size\nexample_inputs = torch.rand((batch_size, 28, 28))\n\nonnx_program = torch.onnx.export(torch_model,\n                                 example_inputs,\n                                 input_names=['input'],\n                                 output_names=['output'],\n                                 dynamic_axes = { # variable input/output: first dimension, corresponding to batch size\n                                     'input' : {0 : 'batch_size'}, \n                                     'output' : {0 : 'batch_size'}\n                                     },\n                                 f=\"converted_model.onnx\",\n                                 export_params=True,\n                                 do_constant_folding=True # Optimization\n                                 )\n```\n:::\n\n\n## Running The ONNX Model\n\nWe can run *any* ONNX model inside python, using the `onnxruntime` package. Let's run our own model that we just exported by downloading it from `converted_model.onnx`.\n\n::: {#b95c9e75 .cell execution_count=3}\n``` {.python .cell-code}\nimport onnx\n\nmodel = onnx.load(\"converted_model.onnx\")\nonnx.checker.check_model(model) # If this does not raise an error, we can continue\n```\n:::\n\n\nSince ONNX does not support all data types that PyTorch uses, we need to do a bit of pre-processing before we can actually run the model.\n\n\n\n::: {#e6017bb4 .cell execution_count=5}\n``` {.python .cell-code}\nimport onnxruntime as ort\nimport numpy as np\n\n# Define a 'session', which will run the model\nort_session = ort.InferenceSession(\"converted_model.onnx\", providers=[\"CPUExecutionProvider\"])\n\n# The function that will convert PyTorch inputs to ONNX inputs\ndef to_numpy(tensor):\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n\n# Sample image \n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=transforms.ToTensor()\n)\n\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n\nX, y = next(iter(train_dataloader))\ntesting_image = X[0]\ntesting_image_label = y[0]\n\ninput_name = ort_session.get_inputs()[0].name #  We specified this in our onnx_program's input_names parameter\ninput_values = to_numpy(testing_image)\nort_inputs = {input_name: input_values}\n\nort_outputs = ort_session.run(None, ort_inputs)\n```\n:::\n\n\n:::{.callout-tip}\nI specified `None` for the output_names parameter of the `.run()` method. This computes all outputs. If we had specific outputs defined in `onnx_program`'s `output_names`, we could pass them here in a list. In this case, running `.run(output_names=['output'])` would result in the same output.\n:::\n\n## Sanity Checking Output\nUsing ONNX comes with the advantage of a standardized model format which we can run anywhere, but it still needs to give the same output as the model that we trained using PyTorch. Let's make sure that nothing went wrong during conversion by comparing the PyTorch model output and the ONNX model output on the same input:\n\n::: {#acf7026b .cell execution_count=6}\n``` {.python .cell-code}\ntorch_outputs = torch_model(testing_image)\ntorch_outputs = to_numpy(torch_outputs)\n\nnp.testing.assert_allclose(ort_outputs[0], torch_outputs, rtol=1e-03, atol=1e-05) # No error: good to go!\n```\n:::\n\n\n## How To Get ONNX predicted labels?\nWe can get the labels back by using `np.argmax`: \n\n::: {#e49a226f .cell execution_count=7}\n``` {.python .cell-code}\nidx_to_class = {\n    0: 'T-shirt/top',\n    1: 'Trouser',\n    2: 'Pullover',\n    3: 'Dress',\n    4: 'Coat',\n    5: 'Sandal',\n    6: 'Shirt',\n    7: 'Sneaker',\n    8: 'Bag',\n    9: 'Ankle boot'\n}\nlabel_index = np.argmax(ort_outputs[0], axis=1).item()\nclass_label = idx_to_class[label_index]\n```\n:::\n\n\n:::{.callout-tip}\nWhen using a pre-trained model from torchvision.models, we can retrieve the class label through `weights.meta[\"categories\"]`. E.g `ResNet50_Weights.meta[\"categories\"][label_index]`\n:::\n\n## Deploying ONNX models\nIn this section we will deploy our model to [HuggingFace Spaces]()\n\nFor demonstration purposes I will be using a [ResNet-50](https://docs.pytorch.org/vision/stable/models/generated/torchvision.models.resnet50.html) with default weights, saved as a '.onnx' file. \n\nFollow these steps to get started:\n\n1. ### Create an account\nCreate an account at [Hugging Face](https://huggingface.co/login) if you don't have one.\n\n2. ### Create a new space\nSelect 'Gradio' as the Space SDK. Gradio is a high-level API that generates a UI for machine learning models with very few code.\n\n3. ### Generate a password for the Space\nGo to Settings > Access tokens, and scroll down to 'Repositories permissions'. Select your space and click the write permissions.\n\n4. ### Push the app\nWe only need to specify 2 functions to create a UI and do inference. The `predict` function, and a preprocessing function. The last depends on the model that you are using. PyTorch pre-trained models also have their required preprocessing made available through `{weights_name.VERSION}.transforms()`:\n\n::: {#d27f859d .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show imports\"}\nimport numpy as np\nimport onnxruntime as ort\nimport gradio as gr\nfrom PIL import Image\nfrom torchvision.models import ResNet50_Weights\n```\n:::\n\n\n::: {#45a7d15c .cell execution_count=9}\n``` {.python .cell-code}\nweights = ResNet50_Weights.DEFAULT\npreprocess = weights.transforms() # Necessary input transformations\nort_session = ort.InferenceSession(\"resnet50.onnx\", providers=[\"CPUExecutionProvider\"])\n\ndef preprocess_inputs(img: Image):\n    img = preprocess(img) # Change this line when using a different model\n    img_array = np.array(img).astype(np.float32)\n    img_array = np.expand_dims(img_array, axis=0)\n    return img_array\n\ndef predict(img):\n    img = preprocess_inputs(img)\n    ort_inputs = {ort_session.get_inputs()[0].name: img}\n    ort_outputs = ort_session.run(None, ort_inputs)\n\n    label_index = np.argmax(ort_outputs[0], axis=1).item()\n    predicted_label = weights.meta[\"categories\"][label_index]\n    return predicted_label\n```\n:::\n\n\nThat's it! Now we can build the interface:\n\n::: {#64e32655 .cell execution_count=10}\n``` {.python .cell-code}\ndemo = gr.Interface(predict, gr.Image(type=\"pil\", image_mode=\"RGB\"), gr.Label())\ndemo.launch()\n```\n:::\n\n\nYour file structure should look like this:\n\n::: {#17c960d9 .cell execution_count=11}\n\n::: {.cell-output .cell-output-stdout}\n```\n.\n├── README.md\n├── app.py\n├── requirements.txt\n└── resnet50.onnx\n```\n:::\n:::\n\n\nWhen all the code is in `app.py` and your project dependencies (imports) are listed in a `requirements.txt` you are ready to push and deploy. Run `git push`. If you encounter an error, you will need to [install git-lfs](https://docs.github.com/en/repositories/working-with-files/managing-large-files/installing-git-large-file-storage)\n\n## Further Reading\n\n- [Official Gradio Quickstart](https://www.gradio.app/guides/quickstart)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}