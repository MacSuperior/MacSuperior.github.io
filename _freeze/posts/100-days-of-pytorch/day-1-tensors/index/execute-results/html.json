{
  "hash": "de8e986498a408ac82a05aea4c5ceaa3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Pytorch Basics: Tensors'\ndescription: Tensors lie at the heart of PyTorch. If we want to be proficient in PyTorch, we need to know what we can do with the `Tensor` object. There are over 1200 possible operations to perform on tensors, but only a few are needed to get started with PyTorch. In this post we learn what a Tensor is and how to perform basic operations with them. Familiarity with python programming is assumed.\ndate: '2025-05-08'\n---\n\n## What is a tensor?\nIf you are familiar with math, it might help to think about a tensor as an n-dimensional matrix. If you are not familiar with math, or just want a better explanation, you can think of a tensor as a collection of structured numbers that we can do *quick math* with. A tensor has two properties: shape, and dimension. *Shape* means how many numbers the tensor has along each axis. *Dimension* means the amount of axes that the tensor has. In the picture below, the dimension corresponds with the number of colored arrows, and the shape is denoted below the tensor.\n\n![A vector (1D), a matrix (2D) and a 3 dimensional tensor (3D) are all tensors.](./images/tensor.png)\n\n\n## How to create a tensor?\nFirst, `import torch`. After that we can create a tensor in two ways. From existing data, or with new data.\n\n\n\n::: {#ebf16651 .cell execution_count=2}\n``` {.python .cell-code}\n# Create a tensor from existing data\ndata = [[1, 2], [3, 4]]\ntensor_from_data = torch.tensor(data)\nprint(tensor_from_data)\n\n# Create a tensor with new data\nones_tensor = torch.ones((2,2))\nprint(ones_tensor)\n```\n:::\n\n\n<details>\n<summary>Show output</summary>\n\n::: {#4d7735a7 .cell execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[1, 2],\n        [3, 4]])\ntensor([[1., 1.],\n        [1., 1.]])\n```\n:::\n:::\n\n\n</details>\n\n\n## How to do math with tensors?\nThere are three ways to perform a math operation in PyTorch. Lets see an example with addition.\n\n::: {#b3258c87 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Initialization code\"}\n# Initialize tensors to do math with\nshape = (2,2)\ntensor1 = torch.rand(shape)\ntensor2 = torch.ones(shape)\n```\n:::\n\n\n::: {#b2d7f10a .cell execution_count=5}\n``` {.python .cell-code}\n# 1. Python operators\ntensor1 + tensor2\n\n# 2. Built-in tensor method\ntensor1.add(tensor2)\n\n# 3. Output tensor\noutput_tensor = torch.zeros(shape)\ntorch.add(tensor1, tensor2, out=output_tensor)\n```\n:::\n\n\n:::{.callout-note appearance=\"default\"}\nCurrently, I am not sure about the difference between these three methods. I imagine that in a situation where we need to choose between these, one of the three method will feel most natural to use.\n:::\n\n## Some built-in tensor methods\n\nThere are over 1200 methods that we can perform on tensors. They can all be found at the [PyTorch documentation](https://docs.pytorch.org/docs/stable/torch.html). I skimmed through them and will give my beginner-opinion on which ones I believe will allow us to get a good start.\n\n\n<details>\n<summary><span class=\"foldable-h3\">Mutating and indexing tensors</span></summary>\nKnowing how to select and change individual elements as well as groups of elements is an essential skill to have, and easily learned. Indexing tensors works a lot like python list and numpy array indexing.\n\n::: {#e3c1445d .cell execution_count=6}\n``` {.python .cell-code}\ntensor = torch.rand((3, 3))\n\nfirst_row = tensor[0]\nfirst_two_row = tensor[:2]\nfirst_col = tensor[:, 0]\nfirst_two_col = tensor[:, :2]\n```\n:::\n\n\n<details>\n<summary>Show output</summary>\n\n::: {#d5f1484e .cell execution_count=7}\n\n::: {.cell-output .cell-output-stdout}\n```\nEntire tensor:\ntensor([[0.8942, 0.8148, 0.9423],\n        [0.9512, 0.4707, 0.2919],\n        [0.4433, 0.1706, 0.8242]]) \n\nfirst row:\ntensor([0.8942, 0.8148, 0.9423]) \n\nfirst two rows:\ntensor([[0.8942, 0.8148, 0.9423],\n        [0.9512, 0.4707, 0.2919]]) \n\nfirst column:\ntensor([0.8942, 0.9512, 0.4433]) \n\nfirst two columns:\ntensor([[0.8942, 0.8148],\n        [0.9512, 0.4707],\n        [0.4433, 0.1706]])\n```\n:::\n:::\n\n\n</details>\n\nLets now look at how to combine multiple tensors.\n\n::: {#7495b45a .cell execution_count=8}\n``` {.python .cell-code}\ntensor_ones = torch.ones((3,3))\ntensor_ones[:, 0] = 4\n\n# Combine multiple tensors horizontally\nwide_combine = torch.cat((tensor, tensor_ones), dim=1)\neven_wider_combine = torch.cat((tensor, tensor_ones, tensor_ones), dim=1)\n\n# Combine multiple tensors vertically\nhigh_combine = torch.cat((tensor, tensor_ones), dim=0)\n```\n:::\n\n\n<details>\n<summary>Show output</summary>\n\n::: {#b8c48840 .cell execution_count=9}\n\n::: {.cell-output .cell-output-stdout}\n```\nHorizontal combine:\ntensor([[0.8942, 0.8148, 0.9423, 4.0000, 1.0000, 1.0000],\n        [0.9512, 0.4707, 0.2919, 4.0000, 1.0000, 1.0000],\n        [0.4433, 0.1706, 0.8242, 4.0000, 1.0000, 1.0000]]) \n\nWe can combine any number of tensors we want:\ntensor([[0.8942, 0.8148, 0.9423, 4.0000, 1.0000, 1.0000, 4.0000, 1.0000, 1.0000],\n        [0.9512, 0.4707, 0.2919, 4.0000, 1.0000, 1.0000, 4.0000, 1.0000, 1.0000],\n        [0.4433, 0.1706, 0.8242, 4.0000, 1.0000, 1.0000, 4.0000, 1.0000, 1.0000]]) \n\nVertical combine\ntensor([[0.8942, 0.8148, 0.9423],\n        [0.9512, 0.4707, 0.2919],\n        [0.4433, 0.1706, 0.8242],\n        [4.0000, 1.0000, 1.0000],\n        [4.0000, 1.0000, 1.0000],\n        [4.0000, 1.0000, 1.0000]])\n```\n:::\n:::\n\n\n</details>\n</details>\n\n<details>\n<summary><span class=\"foldable-h3\">Comparing tensors</span></summary>\nJust like with the math operations, we have different notation options to choose from when comparing tensors.\n\n::: {#e0e72d41 .cell execution_count=10}\n``` {.python .cell-code}\ntensor1 = torch.zeros((2,2))\ntensor1[0][0] = 10\ntensor2 = torch.ones((2,2))\n\n# Check if two tensors are equal\ntorch.equal(tensor1, tensor2)\ntensor1 == tensor2\n\n# Check if one tensor is greater or equal to another tensor\ntorch.greater_equal(tensor1, tensor2)\ntorch.ge(tensor1, tensor2)\ntensor1 >= tensor2\n```\n:::\n\n\n<details>\n<summary>Show output</summary>\n\n::: {#675c8420 .cell execution_count=11}\n\n::: {.cell-output .cell-output-stdout}\n```\nTensor 1 equals tensor 2:\ntensor([[False, False],\n        [False, False]]) \n\nTensor 1 >= tensor 2:\ntensor([[ True, False],\n        [False, False]])\n```\n:::\n:::\n\n\n</details>\n\nOther comparison operators are implemented like the ones shown above in the way you probably expect. If you can't find the one you're looking for, there exists a [list of all comparison operators]((https://docs.pytorch.org/docs/stable/torch.html#comparison-ops)) on the PyTorch website.\n</details>\n\n<details>\n<summary><span class=\"foldable-h3\">More creation methods</span></summary>\nBeing able to instantiate a tensor with other values than ones and zeros is also possible.\n\n::: {#a37984d3 .cell execution_count=12}\n``` {.python .cell-code}\n# Create a tensor filled with the number 3\nthrees = torch.full(size=(2,2), fill_value=3)\n\n# Create a tensor based on the shape of another\ntensor_with_shape = torch.rand((4, 3))\ntensor_zeros = torch.zeros_like(tensor_with_shape)\n```\n:::\n\n\n<details>\n<summary>Show output</summary>\n\n::: {#550a2ca7 .cell execution_count=13}\n\n::: {.cell-output .cell-output-stdout}\n```\nthrees:\ntensor([[3, 3],\n        [3, 3]]) \n\ntensor_with_shape:\ntensor([[0.5788, 0.5643, 0.9288],\n        [0.8084, 0.8708, 0.4467],\n        [0.6463, 0.9904, 0.3890],\n        [0.2245, 0.4862, 0.8490]]) \n\ntensor_zeros:\ntensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\n```\n:::\n:::\n\n\n</details>\n\n::: {.callout-tip}\nThe way we created `tensor_zeros` based on the shape of another tensor using `zeros_like` can also be done to the other ways we learned how to create a tensor, just by suffixing the method with `_like`.\n:::\n\n### Random numbers\n\n::: {#afe9fd04 .cell execution_count=14}\n``` {.python .cell-code}\nshape = (3,3)\n# Set an optional seed for reproducibility\ntorch.manual_seed(1)\n\n# Random integers \nrand_int = torch.randint(10, shape)\nrand_int_lower_bound = torch.randint(8, 10, shape)\n\n# Tensor values drawn from a distribution\nnormal = torch.randn(shape)\n\nprobability_tensor = torch.empty(shape).uniform_(0, 1) # The '_' suffix modifies the variable in place\nbernoulli =torch.bernoulli(probability_tensor) # Pass a tensor with values of how likely a '1' is.\n\n# Shuffle numbers from 0 to n-1 \npermutation = torch.randperm(10)\n```\n:::\n\n\n<details>\n<summary>Show output</summary>\n\n::: {#441cb490 .cell execution_count=15}\n\n::: {.cell-output .cell-output-stdout}\n```\nrand_int):\ntensor([[5, 9, 4],\n        [8, 3, 3],\n        [1, 1, 9]]) \n\nrand_int_lower_bound:\ntensor([[8, 8, 9],\n        [8, 9, 9],\n        [8, 8, 9]]) \n\nnormal:\ntensor([[-1.1948,  0.0250, -0.7627],\n        [ 1.3969, -0.3245,  0.2879],\n        [ 1.0579,  0.9621,  0.3935]]) \n\nprobability_tensor: \n\ntensor([[0.7140, 0.2676, 0.9906],\n        [0.2885, 0.8750, 0.5059],\n        [0.2366, 0.7570, 0.2346]])\nbernoulli:\ntensor([[1., 0., 1.],\n        [1., 1., 0.],\n        [0., 0., 0.]]) \n\npermutation:\ntensor([1, 9, 3, 6, 8, 0, 5, 2, 7, 4]) \n\n```\n:::\n:::\n\n\n</details>\n\n</details>\n\nThank you for reading! Note that these are just the basics. Now is your time to do some work yourself. Read the documentation, and try some basic operations. As a starter, this post did not cover the different [linear algebra](https://docs.pytorch.org/docs/stable/linalg.html) operations, even though they are very useful! You might also peek at the more technical [GPU operations](https://docs.pytorch.org/docs/stable/cuda.html). See you on day 2!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}