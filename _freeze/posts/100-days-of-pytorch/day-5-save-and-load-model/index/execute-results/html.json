{
  "hash": "8cafff0ad66b9d66344fa584867d1d87",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Day 5: PyTorch Basics - Saving and Loading Models\"\npagetitle: \"PyTorch Basics - Saving and Loading Models\"\ndescription: \"Previously, we learned how to train a neural network model. But how do we actually make predictions with it? And how can we use open-source models that other people trained? In this post we learn how to store and load the weights that the model learned during training, and how to use open-source models and weights\"\ndescription-meta: \"In this post we learn how to store and load the weights that the model learned during training, and how to use open-source models and weights, using torchvision.models\"\nimage: \"thumbnail.jpg\"\ndate: \"2025-05-12\"\ndate-meta: \"2025-05-12\"\n\nkeywords: [Save PyTorch model, Load PyTorch model, Open-Source Models, Pre Trained Model]\njupyter: python3\n---\n\n*This post is part of a series on deploying models, other posts include:*\n\n- [Deploying a model to HuggingFace Spaces through ONNX](../day-6-deploying-model-to-huggingface-spaces-through-onnx/index.qmd)\n\n## How to Save a PyTorch Model?\nLet's reuse our model training code from the [previous blog post](../day-4-building-and-training-a-neural-network/index.qmd).\n\n::: {#8e8c28fb .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show code\"}\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import ToTensor\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)\n\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n\nclass OurNeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Define neural net structure here, so we can store weights in them.\n        self.flatten = nn.Flatten()\n        self.linear_relu_chain = nn.Sequential(\n            nn.Linear(in_features=28*28, out_features=512),\n            nn.ReLU(),\n            nn.Linear(in_features=512, out_features=512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n    \n    def forward(self, input):\n        # Use neural net structure to pass input data through\n\n        input = self.flatten(input) # Shape: (28,28) -> shape: (784)\n\n        predictions = self.linear_relu_chain(input) # Shape: (784) -> shape: (512) -> shape: (512) -> shape: (10)\n        \n        return predictions\n\ndef train_loop(dataloader, model, loss_func, optimizer):\n    size = len(dataloader.dataset)\n    model.train() # Set model to training mode\n\n    # Update parameters each new batch\n    for batch, (images, labels) in enumerate(dataloader):\n        model_predictions = model(images)\n        loss = loss_func(model_predictions, labels)\n\n        # Compute gradient with backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        # Something to look at while model trains\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * batch_size + len(images)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n\ndef checking_loop(dataloader, model, loss_func):\n    size = len(dataloader.dataset)\n    number_of_batches = len(dataloader)\n    test_loss, correct_amount = 0, 0\n\n    model.eval() # Set model to check/test mode\n\n    with torch.no_grad(): # We don't need to update parameters anymore. This speeds up testing.\n\n        # This dataloader contains the test images\n        for images, labels in dataloader:\n            model_predictions = model(images)\n            \n            loss = loss_func(model_predictions, labels).item()\n            test_loss += loss\n\n            predicted_labels = nn.Softmax(dim=1)(model_predictions).argmax(1)\n            correct = predicted_labels == labels\n            # Turn every 'True' into a 1, and sum over them, converting the resulting tensor to a python integer\n            correct_amount += correct.type(torch.float).sum().item()\n\n    test_loss /= number_of_batches\n    correct_amount /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct_amount):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n\n\nmodel = OurNeuralNetwork().to(\"cpu\")\n\nlearning_rate = 1e-3\nbatch_size = 64\nepochs = 5\n\nloss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n```\n:::\n\n\nTraining the model again:\n\n::: {#20732fe2 .cell execution_count=2}\n``` {.python .cell-code}\nepochs = 10\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train_dataloader, model, loss_function, optimizer)\n    checking_loop(test_dataloader, model, loss_function)\nprint(\"Done!\")\n```\n:::\n\n\n<details>\n<summary>Show output</summary>\n\n::: {#b5c24d43 .cell execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nEpoch 1\n-------------------------------\nloss: 1.469673  [   64/60000]\nloss: 1.433510  [ 6464/60000]\nloss: 1.416146  [12864/60000]\nloss: 1.377274  [19264/60000]\nloss: 1.353796  [25664/60000]\nloss: 1.429164  [32064/60000]\nloss: 1.270087  [38464/60000]\nloss: 1.251621  [44864/60000]\nloss: 1.361366  [51264/60000]\nloss: 1.273360  [57664/60000]\nTest Error: \n Accuracy: 63.7%, Avg loss: 1.244470 \n\nEpoch 2\n-------------------------------\nloss: 1.250782  [   64/60000]\nloss: 1.358424  [ 6464/60000]\nloss: 1.121522  [12864/60000]\nloss: 1.036822  [19264/60000]\nloss: 1.188575  [25664/60000]\nloss: 1.127103  [32064/60000]\nloss: 1.138114  [38464/60000]\nloss: 1.059073  [44864/60000]\nloss: 1.033557  [51264/60000]\nloss: 1.098557  [57664/60000]\nTest Error: \n Accuracy: 65.0%, Avg loss: 1.083130 \n\nEpoch 3\n-------------------------------\nloss: 1.070835  [   64/60000]\nloss: 1.022191  [ 6464/60000]\nloss: 1.005594  [12864/60000]\nloss: 0.993012  [19264/60000]\nloss: 1.047417  [25664/60000]\nloss: 1.001495  [32064/60000]\nloss: 1.095251  [38464/60000]\nloss: 0.926997  [44864/60000]\nloss: 0.960782  [51264/60000]\nloss: 0.937367  [57664/60000]\nTest Error: \n Accuracy: 66.1%, Avg loss: 0.979457 \n\nEpoch 4\n-------------------------------\nloss: 0.900051  [   64/60000]\nloss: 1.099103  [ 6464/60000]\nloss: 1.052053  [12864/60000]\nloss: 0.843110  [19264/60000]\nloss: 0.914962  [25664/60000]\nloss: 1.017330  [32064/60000]\nloss: 0.707650  [38464/60000]\nloss: 0.890666  [44864/60000]\nloss: 1.078490  [51264/60000]\nloss: 0.758047  [57664/60000]\nTest Error: \n Accuracy: 67.3%, Avg loss: 0.909492 \n\nEpoch 5\n-------------------------------\nloss: 0.935071  [   64/60000]\nloss: 0.930360  [ 6464/60000]\nloss: 0.886458  [12864/60000]\nloss: 0.747989  [19264/60000]\nloss: 0.919060  [25664/60000]\nloss: 0.857149  [32064/60000]\nloss: 0.808115  [38464/60000]\nloss: 0.957309  [44864/60000]\nloss: 0.915866  [51264/60000]\nloss: 1.035016  [57664/60000]\nTest Error: \n Accuracy: 68.0%, Avg loss: 0.857042 \n\nEpoch 6\n-------------------------------\nloss: 0.711309  [   64/60000]\nloss: 0.731404  [ 6464/60000]\nloss: 0.778495  [12864/60000]\nloss: 0.826608  [19264/60000]\nloss: 0.690381  [25664/60000]\nloss: 0.793883  [32064/60000]\nloss: 1.049005  [38464/60000]\nloss: 0.860935  [44864/60000]\nloss: 0.850578  [51264/60000]\nloss: 0.894870  [57664/60000]\nTest Error: \n Accuracy: 69.2%, Avg loss: 0.820537 \n\nEpoch 7\n-------------------------------\nloss: 0.751307  [   64/60000]\nloss: 0.690765  [ 6464/60000]\nloss: 0.885832  [12864/60000]\nloss: 0.810388  [19264/60000]\nloss: 0.656271  [25664/60000]\nloss: 0.795354  [32064/60000]\nloss: 0.873639  [38464/60000]\nloss: 0.952544  [44864/60000]\nloss: 0.621379  [51264/60000]\nloss: 0.782824  [57664/60000]\nTest Error: \n Accuracy: 70.0%, Avg loss: 0.791091 \n\nEpoch 8\n-------------------------------\nloss: 0.706885  [   64/60000]\nloss: 0.791194  [ 6464/60000]\nloss: 0.665691  [12864/60000]\nloss: 0.586563  [19264/60000]\nloss: 0.746921  [25664/60000]\nloss: 0.670890  [32064/60000]\nloss: 0.818113  [38464/60000]\nloss: 0.725863  [44864/60000]\nloss: 0.793836  [51264/60000]\nloss: 0.689501  [57664/60000]\nTest Error: \n Accuracy: 71.0%, Avg loss: 0.764419 \n\nEpoch 9\n-------------------------------\nloss: 0.579552  [   64/60000]\nloss: 0.783948  [ 6464/60000]\nloss: 0.766569  [12864/60000]\nloss: 0.831361  [19264/60000]\nloss: 0.964704  [25664/60000]\nloss: 0.772870  [32064/60000]\nloss: 0.836838  [38464/60000]\nloss: 0.806005  [44864/60000]\nloss: 0.795276  [51264/60000]\nloss: 0.934505  [57664/60000]\nTest Error: \n Accuracy: 72.8%, Avg loss: 0.744880 \n\nEpoch 10\n-------------------------------\nloss: 0.493929  [   64/60000]\nloss: 0.790016  [ 6464/60000]\nloss: 0.750857  [12864/60000]\nloss: 0.762535  [19264/60000]\nloss: 0.822756  [25664/60000]\nloss: 0.723158  [32064/60000]\nloss: 0.535035  [38464/60000]\nloss: 0.708430  [44864/60000]\nloss: 0.695287  [51264/60000]\nloss: 0.616080  [57664/60000]\nTest Error: \n Accuracy: 73.5%, Avg loss: 0.724422 \n\nDone!\n```\n:::\n:::\n\n\n</details>\n\nThe learned weights are stored in an attribute called `state_dict`. We can save these weights to a file to reuse them later by calling `torch.save()` on the attribute.\n\n::: {#953fa2c9 .cell execution_count=4}\n``` {.python .cell-code}\ntorch.save(model.state_dict(), \"model_weights.pth\")\n```\n:::\n\n\n## How to Load a Saved Model?\nTo load the saved model, we first need to create a new instance of the same model class. Just the weights won't do us any good, they need to correspond to the correct model structure. After instantiating a new model, we can copy the weights to it.\n\n::: {#dbaffba9 .cell execution_count=5}\n``` {.python .cell-code}\nweights = torch.load(\"model_weights.pth\", weights_only=True)\n\nmodel_from_weights = OurNeuralNetwork()\nmodel_from_weights.load_state_dict(weights)\nmodel_from_weights.eval()\n```\n:::\n\n\n<details><summary>Show output</summary>\n\n::: {#4458bc1b .cell execution_count=6}\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nOurNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_chain): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n```\n:::\n:::\n\n\n</details>\n\nWe now have an exact copy of the neural network in a new variable! \n\nIf we want, we can inspect all the parameters (e.g to create visualizations that explain the model) through the models' `state_dict()`\n\n::: {#0493eafc .cell execution_count=7}\n``` {.python .cell-code}\nlayer_2_bias = model.state_dict()['linear_relu_chain.2.bias'] # We can access stored parameters through the state_dict keys\nprint(\"stored parameters in the form of 'layer_num.type': \", model.state_dict().keys(), '\\n')\nprint(\"Amount of values in the layer 2 bias:\", layer_2_bias.shape, '\\n') \nprint(\"First 10 biases in layer 2:\", layer_2_bias[:10])\n```\n:::\n\n\n::: {#560de6d5 .cell execution_count=8}\n\n::: {.cell-output .cell-output-stdout}\n```\nstored parameters in the form of 'layer_num.type': \n[\n    'linear_relu_chain.0.weight',\n    'linear_relu_chain.0.bias',\n    'linear_relu_chain.2.weight',\n    'linear_relu_chain.2.bias',\n    'linear_relu_chain.4.weight',\n    'linear_relu_chain.4.bias',\n]\nAmount of values in the layer 2 bias: torch.Size([512]) \n\nFirst 3 biases in layer 2: tensor([0.0234, 0.0045, 0.0241])\n```\n:::\n:::\n\n\n## How to Use Pre-Trained Models?\n\nWe don't *have* to train every model that we want to use ourselves. Lots of times, a better model trained on more data for longer is available freely online. PyTorch comes with [pre-built model structures](https://docs.pytorch.org/vision/stable/models.html) and weights for these models.\n\n::: {#8542fb28 .cell execution_count=9}\n``` {.python .cell-code}\nfrom torchvision.models import resnet50, ResNet50_Weights\n\nweights = ResNet50_Weights.DEFAULT\nmodel = resnet50(weights)\n\nmodel.eval() \n```\n:::\n\n\nWe can now use the ResNet50 model with best weights in our code.\n\n## Warmstarting / Transfer Learning\nIn a scenario where we have a dataset with domain-specific images and we want to train an image recognition model on the data, we don't have to start from scratch. We can define the model structure that we want and use imported weights for initializing the model training. This way the model does not have to learn what an image is again. Since that knowledge is already embedded in the downloaded weights, the model only needs to learn to recognize the domain-specific images. [This PyTorch article](https://docs.pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) explains how to do this.\n\n## Further Reading\n\n- [Saving multiple models in one file](https://docs.pytorch.org/tutorials/beginner/saving_loading_models.html#saving-multiple-models-in-one-file)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}