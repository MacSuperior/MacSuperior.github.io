[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I‚Äôm Liam Groen, a student from the Netherlands with a deep interest in machine learning, data science, and mathematics.\nI‚Äôm currently focused on building a strong foundation in ML by combining theory and practice:\nüìò Reading Pattern Recognition and Machine Learning by Bishop\nüß™ Working through PyTorch tutorials and hands-on experiments\nüéØ Committing to learn and build something new every day\nThis blog is where I document what I‚Äôm learning ‚Äî not only as a study aid, but as a way to share insights and connect with others on a similar journey.\n\n\n\n\nBuild solid understanding of machine learning fundamentals\nGet hands-on experience with real-world data and models\nPrepare for internships and a career in applied AI\n\n\n\n\n\nI‚Äôm always open to collaborating, learning together, or hearing from others in the field. Feel free to reach out on LinkedIn or check out my GitHub projects.\n\nVisualizing the optimization terrain of deep networks ‚Äî Li et al.¬†(2018)."
  },
  {
    "objectID": "about.html#my-learning-goals",
    "href": "about.html#my-learning-goals",
    "title": "About",
    "section": "",
    "text": "Build solid understanding of machine learning fundamentals\nGet hands-on experience with real-world data and models\nPrepare for internships and a career in applied AI"
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "About",
    "section": "",
    "text": "I‚Äôm always open to collaborating, learning together, or hearing from others in the field. Feel free to reach out on LinkedIn or check out my GitHub projects.\n\nVisualizing the optimization terrain of deep networks ‚Äî Li et al.¬†(2018)."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDay 3: Pytorch Basics - Transforms\n\n\n\n100 Days Of PyTorch\n\n\n\n\n\n\n\n\n\nLiam Groen\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2: PyTorch Basics - Dataset and DataLoader\n\n\n\n100 Days Of PyTorch\n\n\n\nIn this post we learn about how to download datasets with PyTorch Datasets, and how to retrieve data from them for training an ML model while keeping data and model separate. We will see that working with these classes is pretty easy, and allows us to use all kinds of handy built-in methods.\n\n\n\n\n\nMay 9, 2025\n\n\nLiam Groen\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1: Pytorch Basics - Tensors\n\n\n\n100 Days Of PyTorch\n\n\n\nTensors lie at the heart of PyTorch. If we want to be proficient in PyTorch, we need to know what we can do with the Tensor object. There are over 1200 possible operations to perform on tensors, but only a few are needed to get started with PyTorch. In this post we learn what a Tensor is and how to perform basic operations with them. Familiarity with python programming is assumed.\n\n\n\n\n\nMay 8, 2025\n\n\nLiam Groen\n\n\n\n\n\n\n\n\n\n\n\n\nBackpropagation: How Does Your Computer Calculate Gradients?\n\n\n\nexplanation\n\n\n\nHow backpropagation is used to compute gradients in neural networks, using computational graphs and the chain rule.\n\n\n\n\n\nApr 20, 2025\n\n\nLiam Groen\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/backpropagation/index.html",
    "href": "posts/backpropagation/index.html",
    "title": "Backpropagation: How Does Your Computer Calculate Gradients?",
    "section": "",
    "text": "A neural network consists of initially random weights and biases. Training the network boils down to minimizing a loss function that quantifies how wrong the network‚Äôs output is compared to the desired output. Gradient descent is the algorithm used for finding weights and biases that minimize the loss. The way these parameters are updated is determined by taking a step in the direction of the negative gradient of the loss function, which can be thought of as following an arrow that points towards where the loss function decreases the quickest. The problem is: how do we calculate this gradient?\nA first idea might be to grab a piece of paper and derive the gradient by hand. This is very tedious, requiring lots of matrix calculus and paper, and is infeasible for complex models. Additionally, this solution is not modular, since when you want to change something about the network or loss, you need to recalculate the gradient from scratch. We would like to implement an automatic way of calculating the gradient. Let‚Äôs look at some ways we could do this, excluding backpropagation for now.\n\nNumerical method: Adjust each parameter by a little and see how the loss changes in response. This method lacks precision and does not scale to large neural networks, where it would lead to lots of repeated computations, making it very slow.\nSymbolic method: The thought behind this method is that once an expression for the gradient is found, evaluating it can be fast. This method uses calculus rules to derive an exact expression for the gradient, but is even slower than the numerical method due to large, repeated expressions, making it infeasible to use in practice.\n\nThis is where backpropagation steps in. It brings exact precision while at the same time being extremely quick, making it the standard for updating parameters in neural networks."
  },
  {
    "objectID": "posts/backpropagation/index.html#why-backpropagation",
    "href": "posts/backpropagation/index.html#why-backpropagation",
    "title": "Backpropagation: How Does Your Computer Calculate Gradients?",
    "section": "",
    "text": "A neural network consists of initially random weights and biases. Training the network boils down to minimizing a loss function that quantifies how wrong the network‚Äôs output is compared to the desired output. Gradient descent is the algorithm used for finding weights and biases that minimize the loss. The way these parameters are updated is determined by taking a step in the direction of the negative gradient of the loss function, which can be thought of as following an arrow that points towards where the loss function decreases the quickest. The problem is: how do we calculate this gradient?\nA first idea might be to grab a piece of paper and derive the gradient by hand. This is very tedious, requiring lots of matrix calculus and paper, and is infeasible for complex models. Additionally, this solution is not modular, since when you want to change something about the network or loss, you need to recalculate the gradient from scratch. We would like to implement an automatic way of calculating the gradient. Let‚Äôs look at some ways we could do this, excluding backpropagation for now.\n\nNumerical method: Adjust each parameter by a little and see how the loss changes in response. This method lacks precision and does not scale to large neural networks, where it would lead to lots of repeated computations, making it very slow.\nSymbolic method: The thought behind this method is that once an expression for the gradient is found, evaluating it can be fast. This method uses calculus rules to derive an exact expression for the gradient, but is even slower than the numerical method due to large, repeated expressions, making it infeasible to use in practice.\n\nThis is where backpropagation steps in. It brings exact precision while at the same time being extremely quick, making it the standard for updating parameters in neural networks."
  },
  {
    "objectID": "posts/backpropagation/index.html#the-computational-graph",
    "href": "posts/backpropagation/index.html#the-computational-graph",
    "title": "Backpropagation: How Does Your Computer Calculate Gradients?",
    "section": "The computational graph",
    "text": "The computational graph\nLet‚Äôs see backpropagation in action through a simple example. Let‚Äôs say that we want to minimize the made-up loss function:\n\\[\nL(x, y, z) = (x + y)z\n\\]\nThe way a computer evaluates that function is by constructing a computational graph, consisting of several nodes and edges. It calculates the result by calculating the value at each node and passing it forward to the next node, until it reaches the end of the computation.\nBackpropagation uses this graph, storing intermediate computations that it will later need along the way. This is called the forward pass.\n\nLet‚Äôs name the nodes as follows:\n\\[\n\\begin{align}\n    q &= x + y \\\\\n    L &= zq\n\\end{align}\n\\]\nWe see that the derivatives are\n\\[\n\\frac{\\partial L}{\\partial z} = q,\n\\frac{\\partial L}{\\partial q} = z,\n\\frac{\\partial q}{\\partial x} = 1,\n\\frac{\\partial q}{\\partial y} = 1.\n\\]\nThese represent the effect that \\(z\\) and \\(q\\) have on \\(L\\), and the effect that \\(x\\) and \\(y\\) have on \\(q\\). However, we are not done yet! Remember that our goal was not to find these derivatives, but to find \\(\\frac{\\partial L}{\\partial x}, \\frac{\\partial L}{\\partial y}\\text{ } \\text{and } \\frac{\\partial L}{\\partial z}\\): the effect that \\(x\\), \\(y\\), and \\(z\\) have on the loss \\(L\\) at the end of the graph."
  },
  {
    "objectID": "posts/backpropagation/index.html#calculating-derivatives",
    "href": "posts/backpropagation/index.html#calculating-derivatives",
    "title": "Backpropagation: How Does Your Computer Calculate Gradients?",
    "section": "Calculating derivatives",
    "text": "Calculating derivatives\nLooking at the graph, we see that when the input to \\(q\\) changes by \\(\\partial x\\), its output changes by \\(\\frac{\\partial q}{\\partial x}\\) as a result. How much \\(L\\) changes in response to a change in \\(q\\) is given by \\(\\frac{\\partial L}{\\partial q}\\). So the effect that a change in \\(x\\) has on \\(L\\), is given by chaining these effects together: \\(\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial q}\\frac{\\partial q}{\\partial x}\\).\nFollowing the same reasoning, we see that \\(\\frac{\\partial L}{\\partial y} = \\frac{\\partial L}{\\partial q}\\frac{\\partial q}{\\partial y}\\).\nSince \\(z\\) directly affects \\(L\\) without intermediate nodes, the change in \\(L\\) from a change in \\(z\\) is simply given by \\(\\frac{\\partial L}{\\partial z}\\)\nWe compute these values by moving from the end of the graph back to the beginning, this is called the backward pass.\nDuring the forward pass, every node received input from its upstream nodes, performed a basic computation, and then passed the result forward to its downstream nodes. When the final node in the graph computes its output, the computation is done. At that point the backward pass will start.\n\nNote: A downstream node is one that comes after the flow of data (i.e., closer to the output), and an upstream node is one that comes before (i.e., closer to the input). In the backward pass, since data flows from output to input, the terms are used in reverse."
  },
  {
    "objectID": "posts/backpropagation/index.html#propagating-backward",
    "href": "posts/backpropagation/index.html#propagating-backward",
    "title": "Backpropagation: How Does Your Computer Calculate Gradients?",
    "section": "Propagating backward",
    "text": "Propagating backward\nLet‚Äôs zoom in on a random node during the backward pass.\n\n\n\nA node calculating how its inputs affect the loss, passing these values downstream.\n\n\nEventually, this node will receive a number from its output node, called the upstream gradient. This represents the change in the loss function all the way at the end of the graph when the output of this particular node changes. If the node receives multiple upstream gradients, it sums them up. After receiving the upstream gradient, the node computes local gradients, which represent how much each output of the node is affected by each input to the node. The node then calculates how each of its inputs affects the loss by multiplying each local gradient by the upstream gradient, and passes these downstream gradients to the respective input nodes. These input nodes then receive it as their own upstream gradient, repeating the process.\nThis entire process is a repeated application of the chain rule, which lets us compute how an input affects the final output through intermediate variables. When the process reaches the beginning of the graph, each input received an upstream gradient, meaning that we have the gradient of the loss function with respect to the inputs, and we are able to perform an iteration of gradient descent.\n\nTo summarize, a node:\n\nReceives an upstream gradient\nPasses along downstream gradients by multiplying the upstream gradient with local gradients.\n\nThe only thing left to do is to calculate these local gradients.\nWe do this by defining the local gradient for all node types we have in our graph beforehand.\nIn the case of an addition node, the local gradients are \\(1\\), so the downstream gradients are just the upstream gradient. For multiplication nodes, the local gradient with respect to an input node \\(a\\) is simply the product of all other input nodes. In a two-input case, this means that the gradient with respect to \\(a\\) is simply the other input. Other node types have simple gradient calculations as well!"
  },
  {
    "objectID": "posts/backpropagation/index.html#looking-back",
    "href": "posts/backpropagation/index.html#looking-back",
    "title": "Backpropagation: How Does Your Computer Calculate Gradients?",
    "section": "Looking back",
    "text": "Looking back\nThinking about calculating the gradient in terms of this computational graph has melted away all our initial problems. Remember that calculating the gradient by hand is tedious or infeasible, but using backpropagation it has become trivial! Secondly, we no longer need to recalculate the gradient when we change the structure of the model. This is because the building blocks of the graph remain the same, so whilst the graph could have a different structure, the backpropagation algorithm remains the same, so our modularity issue is solved! Finally, backpropagation does not need to repeat calculations, making it extremely fast, and suitable for training deep neural networks.\nThank you for reading! I recommend that you check out Justin Johnson‚Äôs excellent video, which goes in-depth on generalizing to vector and tensor valued functions, and which this post was based upon."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-3-transforms/index.html",
    "href": "posts/100-days-of-pytorch/day-3-transforms/index.html",
    "title": "Day 3: Pytorch Basics - Transforms",
    "section": "",
    "text": "Data comes in many different formats. On the other hand, PyTorch can only do machine learning with one data type, the tensor. Transforms can convert any data to a tensor. In this post, we will look at how to transform images. I will assume that you are familiar with PyTorch Datasets. If you are not, I recommend reading this post before you continue."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-3-transforms/index.html#why-do-we-need-transform",
    "href": "posts/100-days-of-pytorch/day-3-transforms/index.html#why-do-we-need-transform",
    "title": "Day 3: Pytorch Basics - Transforms",
    "section": "",
    "text": "Data comes in many different formats. On the other hand, PyTorch can only do machine learning with one data type, the tensor. Transforms can convert any data to a tensor. In this post, we will look at how to transform images. I will assume that you are familiar with PyTorch Datasets. If you are not, I recommend reading this post before you continue."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-3-transforms/index.html#how-do-pytorch-transforms-work",
    "href": "posts/100-days-of-pytorch/day-3-transforms/index.html#how-do-pytorch-transforms-work",
    "title": "Day 3: Pytorch Basics - Transforms",
    "section": "How do PyTorch transforms work?",
    "text": "How do PyTorch transforms work?\nAll built-in datasets from the torchvision module take the parameters transform and target_tranform. They take in a function that transforms input data into a tensor, following predefined steps. To avoid having to write these functions ourselves, the torchvision.transforms module come with an image-to-tensor transform, called ToTensor out of the box.\nLet‚Äôs see an example through the FashionMNIST dataset.\n\nimport torch\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision.transforms import ToTensor, Lambda\n\n\ndef our_own_transformation(target):\n    \"\"\"\n    Transformes target label to a one-hot tensor\n    example:\n\n    &gt;&gt;&gt; our_own_transformation(3)\n    &gt;&gt;&gt; torch.tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n    \"\"\"\n\n    zeros_list = torch.zeros(10, dtype=torch.float)\n    one_hot_index = torch.tensor(target)\n    one_hot_tensor = zeros_list.scatter_(0, one_hot_index, value=1)\n    return one_hot_tensor\n\nds_train = FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n    target_transform=Lambda(our_own_transformation)\n)\n\nds_test = FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n    target_transform=Lambda(our_own_transformation)\n)\n\nIn this code, we specified that we want to convert our training data to a tensor using the ToTensor method, and the target label to a tensor using our_own_transformation."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-3-transforms/index.html#further-reading",
    "href": "posts/100-days-of-pytorch/day-3-transforms/index.html#further-reading",
    "title": "Day 3: Pytorch Basics - Transforms",
    "section": "Further reading",
    "text": "Further reading\nThere are many more things we can do with transforms. We can rotate images, shift images, or we can chain transformations together to create a preprocessing pipeline. Since those usecases are too advanced for us at the moment, I will not cover them in this post. However, if you are curious or already more experienced, I recommend that you check out the example section on the Pytorch Website!"
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html",
    "href": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html",
    "title": "PyTorch Basics: Dataset and DataLoader",
    "section": "",
    "text": "According to PyTorch:\n‚ÄúCode for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity.‚Äù\nIn short, we try to prevent messy notebooks and data leakage by seperating our data and data processing from our model. This is done with the Dataset and DataLoader classes."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#why-seperate-classes-anyway",
    "href": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#why-seperate-classes-anyway",
    "title": "PyTorch Basics: Dataset and DataLoader",
    "section": "",
    "text": "According to PyTorch:\n‚ÄúCode for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity.‚Äù\nIn short, we try to prevent messy notebooks and data leakage by seperating our data and data processing from our model. This is done with the Dataset and DataLoader classes."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#how-to-use-the-pytorch-dataset-class",
    "href": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#how-to-use-the-pytorch-dataset-class",
    "title": "PyTorch Basics: Dataset and DataLoader",
    "section": "How to use the PyTorch Dataset class?",
    "text": "How to use the PyTorch Dataset class?\nWe will see that working with these classes is pretty easy. PyTorch has commonly datasets built-in, ready to work with. They are ordered by the task that they are used for, for example, since the fashionMNIST dataset is used for computer vision-related tasks, it is stored in the torchvision module.\nThe fashionMNIST dataset takes 4 parameters:\n\nroot is the folder name where the data will be stored in.\ntrain specifies if you want to download the training or testing data.\ndownload downloads the data from the internet when you don‚Äôt have it locally yet.\ntransform takes a PIL image and transforms it to a tensor.\n\nLet‚Äôs import the fashionMNIST dataset.\n\n\nShow imports\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\n\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)"
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#dataset-attributes",
    "href": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#dataset-attributes",
    "title": "PyTorch Basics: Dataset and DataLoader",
    "section": "Dataset attributes",
    "text": "Dataset attributes\nNow that we saved the fashionMNIST dataset in a Dataset object, what can we do with it?\n\n# Dataset summary \ntraining_data\n\n# Class labels\ntraining_data.classes\n\n# Select a row of data\nimg, label = training_data[0]\n\n\n\nShow output\n\n\n\ntraining_data:\nDataset FashionMNIST\n    Number of datapoints: 60000\n    Root location: data\n    Split: Train\n    StandardTransform\nTransform: ToTensor() \n\ntraining_data.classes:\n[\n  'T-shirt/top',\n  'Trouser',\n  'Pullover',\n  'Dress',\n  'Coat',\n  'Sandal',\n  'Shirt',\n  'Sneaker',\n  'Bag',\n  'Ankle boot',\n] \n\nlabel:\n9\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt‚Äôs always a good idea to inspect your data. Let‚Äôs look at img and label from the first row of data.\n\n\n\nplt.imshow(img.squeeze(), cmap='gray')\nplt.set_title(training_data.classes[label])\n\n\n\n\n\n\n\n\n\n\nThat indeed looks like an ankle boot, very nice!"
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#using-your-own-data-with-dataset",
    "href": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#using-your-own-data-with-dataset",
    "title": "PyTorch Basics: Dataset and DataLoader",
    "section": "Using your own data with Dataset",
    "text": "Using your own data with Dataset\nWe don‚Äôt always want to use predefined datasets. Very often we have our own data that we want to use. Pytorch has two ways of creating your own dataset. the map-style dataset, which is most commonly used, and the iterable-style dataset, for data that comes in on the fly, such as user-log data. The map-style behaves as you would likely expect from a dataset: you know its length beforehand, and you can select data through an index. For this, map-style datasets need to implement the __len__ and __getitem__ method. Let‚Äôs use our own data with a map-style Dataset\nConsider the case where we have a csv file of image file names and the labels associated with them.\n\n\n\n\n\n\n\n\n\nItem Name\nLabel\n\n\n\n\n0\ntshirt1.jpg\n0\n\n\n1\ntshirt2.jpg\n0\n\n\n2\n...\n...\n\n\n3\nankleboot999.jpg\n9\n\n\n\n\n\n\n\nWe would define a custom dataset class as such:\n\nimport os\nimport pandas as pd\nfrom torchvision.io import read_image\n\nclass CustomImageDataSet(Dataset):\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform # optional\n        self.target_transform = target_transform # optional\n    \n    def __len__(self):\n        return len(self.img_labels)\n    \n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n\n        img = read_image(img_path)\n        label = self.img_labels.iloc[idx, 1]\n\n        if self.transform:\n            img = self.transform(img)\n        if self.target_transform:\n            label = self.target_transform(label)\n        return img, label\n\nThe code above is a bit involved, so let‚Äôs walk through it.\n\n__init__ stores values that we pass in variables, and reads the labels file annotations_file.\n__len__ specifies the size of the dataset by returning the amount of labels.\n\n__getitem__ creates a path to an image. For example, if img_dir='images' and idx=0, then img_path is images/tshirt1.jpg. It then reads the image using a predefined PyTorch function, and reads the label. If any transformations are specified they are applied.\nWe can now select images and labels from our dataset, much like we did earlier in @selecting-img-label"
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#how-to-use-the-pytorch-dataloader-class",
    "href": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#how-to-use-the-pytorch-dataloader-class",
    "title": "PyTorch Basics: Dataset and DataLoader",
    "section": "How to use the PyTorch DataLoader class",
    "text": "How to use the PyTorch DataLoader class\nIn the code above we only specified how to return a single (image, label) pair. In practice, we typically use lots of images and labels (called batches) for per training step. Additionally, we want to shuffle data (to prevent overfitting) and we want to speed up the process using multiprocessing. This is where the DataLoader class steps in. Let‚Äôs use the DataLoader to retrieve 64 images at once.\n\nfrom torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n\n# Get data for one simulated training step\ntrain_image_features, train_labels = next(iter(train_dataloader))\n\nRemember that these images are the data used for one training step. Let‚Äôs see the batch of images that our dataloader just sent us.\n\n\nCode\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 8, 8\n\nfor i in range(cols * rows):\n    idx = i\n    img = train_image_features[idx]\n\n    figure.add_subplot(rows, cols, i +1)\n    plt.imshow(img.squeeze(), cmap=\"gray\")\n    plt.title(training_data.classes[train_labels[idx]], pad=1, fontsize=10)\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThat‚Äôs a lot of images! By using DataLoader, we have an easy way to retrieve lots of images from our data at once."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#different-ways-to-shuffle",
    "href": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#different-ways-to-shuffle",
    "title": "PyTorch Basics: Dataset and DataLoader",
    "section": "Different ways to shuffle",
    "text": "Different ways to shuffle\nWe can also control how data is shuffled (or in other words, the way that the random batch is sampled). We do this by setting the sampler parameter of the DataLoader. When doing this we have to set shuffle=False, since the shuffle parameter essentially sets the sampler parameter for us.\n\nimport numpy as np\nfrom torch.utils.data.sampler import SequentialSampler, SubsetRandomSampler\n\n# Returns images in order.\n# The first batch will have the first 32 images, the second batch will have image 33-64, etc.\ntrain_loader = DataLoader(training_data, batch_size=32, sampler=SequentialSampler(training_data))\n\n# Sample randomly, only including the images 50-100.\nindices = np.arange(50, 101)\ntrain_loader = DataLoader(training_data, batch_size=32, sampler=SubsetRandomSampler(indices))\n\nThese DataLoaders will select images in order of appearance in data, and randomly sample a subset of the data. Read about all the ways to sample in the PyTorch documentation.\nTo summarize: With Dataset and DataLoader, PyTorch makes it easy to manage data efficiently and flexibly. In future posts, we‚Äôll explore how these tools integrate into full training loops."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#further-reading",
    "href": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#further-reading",
    "title": "PyTorch Basics: Dataset and DataLoader",
    "section": "Further Reading",
    "text": "Further Reading\nPyTorch also offers ways to speed up sampling through multiprocessing and memory-pinning, which are both reasonably complicated and have some warnings attached to them, the latter being the reason that I did not include them in this post. If you are interested, or already know all about multiprocessing and GPU computations you can read about the topics here. Thanks for reading all the way to the end, I hope to see you on day 3!"
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-1-tensors/index.html",
    "href": "posts/100-days-of-pytorch/day-1-tensors/index.html",
    "title": "Pytorch Basics: Tensors",
    "section": "",
    "text": "If you are familiar with math, it might help to think about a tensor as an n-dimensional matrix. If you are not familiar with math, or just want a better explanation, you can think of a tensor as a collection of structured numbers that we can do quick math with. A tensor has two properties: shape, and dimension. Shape means how many numbers the tensor has along each axis. Dimension means the amount of axes that the tensor has. In the picture below, the dimension corresponds with the number of colored arrows, and the shape is denoted below the tensor.\n\n\n\nA vector (1D), a matrix (2D) and a 3 dimensional tensor (3D) are all tensors."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-1-tensors/index.html#what-is-a-tensor",
    "href": "posts/100-days-of-pytorch/day-1-tensors/index.html#what-is-a-tensor",
    "title": "Pytorch Basics: Tensors",
    "section": "",
    "text": "If you are familiar with math, it might help to think about a tensor as an n-dimensional matrix. If you are not familiar with math, or just want a better explanation, you can think of a tensor as a collection of structured numbers that we can do quick math with. A tensor has two properties: shape, and dimension. Shape means how many numbers the tensor has along each axis. Dimension means the amount of axes that the tensor has. In the picture below, the dimension corresponds with the number of colored arrows, and the shape is denoted below the tensor.\n\n\n\nA vector (1D), a matrix (2D) and a 3 dimensional tensor (3D) are all tensors."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-1-tensors/index.html#how-to-create-a-tensor",
    "href": "posts/100-days-of-pytorch/day-1-tensors/index.html#how-to-create-a-tensor",
    "title": "Pytorch Basics: Tensors",
    "section": "How to create a tensor?",
    "text": "How to create a tensor?\nFirst, import torch. After that we can create a tensor in two ways. From existing data, or with new data.\n\n# Create a tensor from existing data\ndata = [[1, 2], [3, 4]]\ntensor_from_data = torch.tensor(data)\nprint(tensor_from_data)\n\n# Create a tensor with new data\nones_tensor = torch.ones((2,2))\nprint(ones_tensor)\n\n\n\nShow output\n\n\n\ntensor([[1, 2],\n        [3, 4]])\ntensor([[1., 1.],\n        [1., 1.]])"
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-1-tensors/index.html#how-to-do-math-with-tensors",
    "href": "posts/100-days-of-pytorch/day-1-tensors/index.html#how-to-do-math-with-tensors",
    "title": "Pytorch Basics: Tensors",
    "section": "How to do math with tensors?",
    "text": "How to do math with tensors?\nThere are three ways to perform a math operation in PyTorch. Lets see an example with addition.\n\n\nInitialization code\n# Initialize tensors to do math with\nshape = (2,2)\ntensor1 = torch.rand(shape)\ntensor2 = torch.ones(shape)\n\n\n\n# 1. Python operators\ntensor1 + tensor2\n\n# 2. Built-in tensor method\ntensor1.add(tensor2)\n\n# 3. Output tensor\noutput_tensor = torch.zeros(shape)\ntorch.add(tensor1, tensor2, out=output_tensor)\n\n\n\n\n\n\n\nNote\n\n\n\nCurrently, I am not sure about the difference between these three methods. I imagine that in a situation where we need to choose between these, one of the three method will feel most natural to use."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-1-tensors/index.html#some-built-in-tensor-methods",
    "href": "posts/100-days-of-pytorch/day-1-tensors/index.html#some-built-in-tensor-methods",
    "title": "Pytorch Basics: Tensors",
    "section": "Some built-in tensor methods",
    "text": "Some built-in tensor methods\nThere are over 1200 methods that we can perform on tensors. They can all be found at the PyTorch documentation. I skimmed through them and will give my beginner-opinion on which ones I believe will allow us to get a good start.\n\n\nMutating and indexing tensors\n\nKnowing how to select and change individual elements as well as groups of elements is an essential skill to have, and easily learned. Indexing tensors works a lot like python list and numpy array indexing.\n\ntensor = torch.rand((3, 3))\n\nfirst_row = tensor[0]\nfirst_two_row = tensor[:2]\nfirst_col = tensor[:, 0]\nfirst_two_col = tensor[:, :2]\n\n\n\nShow output\n\n\n\nEntire tensor:\ntensor([[0.8942, 0.8148, 0.9423],\n        [0.9512, 0.4707, 0.2919],\n        [0.4433, 0.1706, 0.8242]]) \n\nfirst row:\ntensor([0.8942, 0.8148, 0.9423]) \n\nfirst two rows:\ntensor([[0.8942, 0.8148, 0.9423],\n        [0.9512, 0.4707, 0.2919]]) \n\nfirst column:\ntensor([0.8942, 0.9512, 0.4433]) \n\nfirst two columns:\ntensor([[0.8942, 0.8148],\n        [0.9512, 0.4707],\n        [0.4433, 0.1706]])\n\n\n\nLets now look at how to combine multiple tensors.\n\ntensor_ones = torch.ones((3,3))\ntensor_ones[:, 0] = 4\n\n# Combine multiple tensors horizontally\nwide_combine = torch.cat((tensor, tensor_ones), dim=1)\neven_wider_combine = torch.cat((tensor, tensor_ones, tensor_ones), dim=1)\n\n# Combine multiple tensors vertically\nhigh_combine = torch.cat((tensor, tensor_ones), dim=0)\n\n\n\nShow output\n\n\n\nHorizontal combine:\ntensor([[0.8942, 0.8148, 0.9423, 4.0000, 1.0000, 1.0000],\n        [0.9512, 0.4707, 0.2919, 4.0000, 1.0000, 1.0000],\n        [0.4433, 0.1706, 0.8242, 4.0000, 1.0000, 1.0000]]) \n\nWe can combine any number of tensors we want:\ntensor([[0.8942, 0.8148, 0.9423, 4.0000, 1.0000, 1.0000, 4.0000, 1.0000, 1.0000],\n        [0.9512, 0.4707, 0.2919, 4.0000, 1.0000, 1.0000, 4.0000, 1.0000, 1.0000],\n        [0.4433, 0.1706, 0.8242, 4.0000, 1.0000, 1.0000, 4.0000, 1.0000, 1.0000]]) \n\nVertical combine\ntensor([[0.8942, 0.8148, 0.9423],\n        [0.9512, 0.4707, 0.2919],\n        [0.4433, 0.1706, 0.8242],\n        [4.0000, 1.0000, 1.0000],\n        [4.0000, 1.0000, 1.0000],\n        [4.0000, 1.0000, 1.0000]])\n\n\n\n\n\n\nComparing tensors\n\nJust like with the math operations, we have different notation options to choose from when comparing tensors.\n\ntensor1 = torch.zeros((2,2))\ntensor1[0][0] = 10\ntensor2 = torch.ones((2,2))\n\n# Check if two tensors are equal\ntorch.equal(tensor1, tensor2)\ntensor1 == tensor2\n\n# Check if one tensor is greater or equal to another tensor\ntorch.greater_equal(tensor1, tensor2)\ntorch.ge(tensor1, tensor2)\ntensor1 &gt;= tensor2\n\n\n\nShow output\n\n\n\nTensor 1 equals tensor 2:\ntensor([[False, False],\n        [False, False]]) \n\nTensor 1 &gt;= tensor 2:\ntensor([[ True, False],\n        [False, False]])\n\n\n\nOther comparison operators are implemented like the ones shown above in the way you probably expect. If you can‚Äôt find the one you‚Äôre looking for, there exists a list of all comparison operators on the PyTorch website.\n\n\n\nMore creation methods\n\nBeing able to instantiate a tensor with other values than ones and zeros is also possible.\n\n# Create a tensor filled with the number 3\nthrees = torch.full(size=(2,2), fill_value=3)\n\n# Create a tensor based on the shape of another\ntensor_with_shape = torch.rand((4, 3))\ntensor_zeros = torch.zeros_like(tensor_with_shape)\n\n\n\nShow output\n\n\n\nthrees:\ntensor([[3, 3],\n        [3, 3]]) \n\ntensor_with_shape:\ntensor([[0.5788, 0.5643, 0.9288],\n        [0.8084, 0.8708, 0.4467],\n        [0.6463, 0.9904, 0.3890],\n        [0.2245, 0.4862, 0.8490]]) \n\ntensor_zeros:\ntensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe way we created tensor_zeros based on the shape of another tensor using zeros_like can also be done to the other ways we learned how to create a tensor, just by suffixing the method with _like.\n\n\n\nRandom numbers\n\nshape = (3,3)\n# Set an optional seed for reproducibility\ntorch.manual_seed(1)\n\n# Random integers \nrand_int = torch.randint(10, shape)\nrand_int_lower_bound = torch.randint(8, 10, shape)\n\n# Tensor values drawn from a distribution\nnormal = torch.randn(shape)\n\nprobability_tensor = torch.empty(shape).uniform_(0, 1) # The '_' suffix modifies the variable in place\nbernoulli =torch.bernoulli(probability_tensor) # Pass a tensor with values of how likely a '1' is.\n\n# Shuffle numbers from 0 to n-1 \npermutation = torch.randperm(10)\n\n\n\nShow output\n\n\n\nrand_int):\ntensor([[5, 9, 4],\n        [8, 3, 3],\n        [1, 1, 9]]) \n\nrand_int_lower_bound:\ntensor([[8, 8, 9],\n        [8, 9, 9],\n        [8, 8, 9]]) \n\nnormal:\ntensor([[-1.1948,  0.0250, -0.7627],\n        [ 1.3969, -0.3245,  0.2879],\n        [ 1.0579,  0.9621,  0.3935]]) \n\nprobability_tensor: \n\ntensor([[0.7140, 0.2676, 0.9906],\n        [0.2885, 0.8750, 0.5059],\n        [0.2366, 0.7570, 0.2346]])\nbernoulli:\ntensor([[1., 0., 1.],\n        [1., 1., 0.],\n        [0., 0., 0.]]) \n\npermutation:\ntensor([1, 9, 3, 6, 8, 0, 5, 2, 7, 4]) \n\n\n\n\n\nThank you for reading! Note that these are just the basics. Now is your time to do some work yourself. Read the documentation, and try some basic operations. As a starter, this post did not cover the different linear algebra operations, even though they are very useful! You might also peek at the more technical GPU operations. See you on day 2!"
  },
  {
    "objectID": "100-days-of-pytorch.html",
    "href": "100-days-of-pytorch.html",
    "title": "100 Days Of PyTorch",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nDay 3: Pytorch Basics - Transforms\n\n\n\n\n\n\n\n\nLiam Groen\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2: PyTorch Basics - Dataset and DataLoader\n\n\nIn this post we learn about how to download datasets with PyTorch Datasets, and how to retrieve data from them for training an ML model while keeping data and model separate. We will see that working with these classes is pretty easy, and allows us to use all kinds of handy built-in methods.\n\n\n\n\n\nMay 9, 2025\n\n\nLiam Groen\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1: Pytorch Basics - Tensors\n\n\nTensors lie at the heart of PyTorch. If we want to be proficient in PyTorch, we need to know what we can do with the Tensor object. There are over 1200 possible operations to perform on tensors, but only a few are needed to get started with PyTorch. In this post we learn what a Tensor is and how to perform basic operations with them. Familiarity with python programming is assumed.\n\n\n\n\n\nMay 8, 2025\n\n\nLiam Groen\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Hi, I‚Äôm Liam, a student on a journey to understand the world through machine learning, coding, data science, and mathematics.\nThis blog is my digital lab notebook ‚Äî a place where I explain what I‚Äôm learning, work through complex topics, and share insights that might help others learning too.\nWhether you‚Äôre just starting out or deep into the field, I hope you find something useful here.\n\n\nWriting is thinking. By distilling ideas into blog posts, I‚Äôm aiming to:\n\nDeepen my understanding of technical topics.\nBuild a consistent habit of learning and reflection.\nCreate a public portfolio of work as I grow in this field.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nLiam Groen\n\n\n\n\n\n\n\n\n\n\n\n\nIn this post we learn about how to download datasets with PyTorch Datasets, and how to retrieve data from them for training an ML model while keeping data and model‚Ä¶\n\n\n\nLiam Groen\n\n\nMay 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTensors lie at the heart of PyTorch. If we want to be proficient in PyTorch, we need to know what we can do with the Tensor object. There are over 1200 possible operations‚Ä¶\n\n\n\nLiam Groen\n\n\nMay 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow backpropagation is used to compute gradients in neural networks, using computational graphs and the chain rule.\n\n\n\nLiam Groen\n\n\nApr 20, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#why-i-blog",
    "href": "index.html#why-i-blog",
    "title": "Home",
    "section": "",
    "text": "Writing is thinking. By distilling ideas into blog posts, I‚Äôm aiming to:\n\nDeepen my understanding of technical topics.\nBuild a consistent habit of learning and reflection.\nCreate a public portfolio of work as I grow in this field."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Home",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nLiam Groen\n\n\n\n\n\n\n\n\n\n\n\n\nIn this post we learn about how to download datasets with PyTorch Datasets, and how to retrieve data from them for training an ML model while keeping data and model‚Ä¶\n\n\n\nLiam Groen\n\n\nMay 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTensors lie at the heart of PyTorch. If we want to be proficient in PyTorch, we need to know what we can do with the Tensor object. There are over 1200 possible operations‚Ä¶\n\n\n\nLiam Groen\n\n\nMay 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow backpropagation is used to compute gradients in neural networks, using computational graphs and the chain rule.\n\n\n\nLiam Groen\n\n\nApr 20, 2025\n\n\n\n\n\n\nNo matching items"
  }
]