[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Liam Groen, a student from the Netherlands with a deep interest in machine learning, data science, and mathematics.\nI’m currently focused on building a strong foundation in ML by combining theory and practice:\n📘 Reading Pattern Recognition and Machine Learning by Bishop\n🧪 Working through PyTorch tutorials and hands-on experiments\n🎯 Committing to learn and build something new every day\nThis blog is where I document what I’m learning — not only as a study aid, but as a way to share insights and connect with others on a similar journey.\n\n\n\n\nBuild solid understanding of machine learning fundamentals\nGet hands-on experience with real-world data and models\nPrepare for internships and a career in applied AI\n\n\n\n\n\nI’m always open to collaborating, learning together, or hearing from others in the field. Feel free to reach out on LinkedIn or check out my GitHub projects.\n\nVisualizing the optimization terrain of deep networks — Li et al. (2018)."
  },
  {
    "objectID": "about.html#my-learning-goals",
    "href": "about.html#my-learning-goals",
    "title": "About",
    "section": "",
    "text": "Build solid understanding of machine learning fundamentals\nGet hands-on experience with real-world data and models\nPrepare for internships and a career in applied AI"
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "About",
    "section": "",
    "text": "I’m always open to collaborating, learning together, or hearing from others in the field. Feel free to reach out on LinkedIn or check out my GitHub projects.\n\nVisualizing the optimization terrain of deep networks — Li et al. (2018)."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDay 4 - PyTorch Basics: Building and Training a Neural Network\n\n\n\n100 Days Of PyTorch\n\n\n\nLearn how to use torch.nn to build the neural network structure, and how to train the model to recognize images, using a training and evaluation loop. It assumes you are already familiar with the theory behind neural networks (i.e loss functions, gradient descent).\n\n\n\n\n\nMay 12, 2025\n\n\nLiam Groen\n\n\n\n\n\n\n\n\n\n\n\n\nDay 3: Pytorch Basics - Transforms\n\n\n\n100 Days Of PyTorch\n\n\n\n\n\n\n\n\n\nMay 10, 2025\n\n\nLiam Groen\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2: PyTorch Basics - Dataset and DataLoader\n\n\n\n100 Days Of PyTorch\n\n\n\nIn this post we learn about how to download datasets with PyTorch Datasets, and how to retrieve data from them for training an ML model while keeping data and model separate. We will see that working with these classes is pretty easy, and allows us to use all kinds of handy built-in methods.\n\n\n\n\n\nMay 9, 2025\n\n\nLiam Groen\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1: Pytorch Basics - Tensors\n\n\n\n100 Days Of PyTorch\n\n\n\nTensors lie at the heart of PyTorch. If we want to be proficient in PyTorch, we need to know what we can do with the Tensor object. There are over 1200 possible operations to perform on tensors, but only a few are needed to get started with PyTorch. In this post we learn what a Tensor is and how to perform basic operations with them. Familiarity with python programming is assumed.\n\n\n\n\n\nMay 8, 2025\n\n\nLiam Groen\n\n\n\n\n\n\n\n\n\n\n\n\nBackpropagation: How Does Your Computer Calculate Gradients?\n\n\n\nexplanation\n\n\n\nHow backpropagation is used to compute gradients in neural networks, using computational graphs and the chain rule.\n\n\n\n\n\nApr 20, 2025\n\n\nLiam Groen\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/backpropagation/index.html",
    "href": "posts/backpropagation/index.html",
    "title": "Backpropagation: How Does Your Computer Calculate Gradients?",
    "section": "",
    "text": "A neural network consists of initially random weights and biases. Training the network boils down to minimizing a loss function that quantifies how wrong the network’s output is compared to the desired output. Gradient descent is the algorithm used for finding weights and biases that minimize the loss. The way these parameters are updated is determined by taking a step in the direction of the negative gradient of the loss function, which can be thought of as following an arrow that points towards where the loss function decreases the quickest. The problem is: how do we calculate this gradient?\nA first idea might be to grab a piece of paper and derive the gradient by hand. This is very tedious, requiring lots of matrix calculus and paper, and is infeasible for complex models. Additionally, this solution is not modular, since when you want to change something about the network or loss, you need to recalculate the gradient from scratch. We would like to implement an automatic way of calculating the gradient. Let’s look at some ways we could do this, excluding backpropagation for now.\n\nNumerical method: Adjust each parameter by a little and see how the loss changes in response. This method lacks precision and does not scale to large neural networks, where it would lead to lots of repeated computations, making it very slow.\nSymbolic method: The thought behind this method is that once an expression for the gradient is found, evaluating it can be fast. This method uses calculus rules to derive an exact expression for the gradient, but is even slower than the numerical method due to large, repeated expressions, making it infeasible to use in practice.\n\nThis is where backpropagation steps in. It brings exact precision while at the same time being extremely quick, making it the standard for updating parameters in neural networks."
  },
  {
    "objectID": "posts/backpropagation/index.html#why-backpropagation",
    "href": "posts/backpropagation/index.html#why-backpropagation",
    "title": "Backpropagation: How Does Your Computer Calculate Gradients?",
    "section": "",
    "text": "A neural network consists of initially random weights and biases. Training the network boils down to minimizing a loss function that quantifies how wrong the network’s output is compared to the desired output. Gradient descent is the algorithm used for finding weights and biases that minimize the loss. The way these parameters are updated is determined by taking a step in the direction of the negative gradient of the loss function, which can be thought of as following an arrow that points towards where the loss function decreases the quickest. The problem is: how do we calculate this gradient?\nA first idea might be to grab a piece of paper and derive the gradient by hand. This is very tedious, requiring lots of matrix calculus and paper, and is infeasible for complex models. Additionally, this solution is not modular, since when you want to change something about the network or loss, you need to recalculate the gradient from scratch. We would like to implement an automatic way of calculating the gradient. Let’s look at some ways we could do this, excluding backpropagation for now.\n\nNumerical method: Adjust each parameter by a little and see how the loss changes in response. This method lacks precision and does not scale to large neural networks, where it would lead to lots of repeated computations, making it very slow.\nSymbolic method: The thought behind this method is that once an expression for the gradient is found, evaluating it can be fast. This method uses calculus rules to derive an exact expression for the gradient, but is even slower than the numerical method due to large, repeated expressions, making it infeasible to use in practice.\n\nThis is where backpropagation steps in. It brings exact precision while at the same time being extremely quick, making it the standard for updating parameters in neural networks."
  },
  {
    "objectID": "posts/backpropagation/index.html#the-computational-graph",
    "href": "posts/backpropagation/index.html#the-computational-graph",
    "title": "Backpropagation: How Does Your Computer Calculate Gradients?",
    "section": "The computational graph",
    "text": "The computational graph\nLet’s see backpropagation in action through a simple example. Let’s say that we want to minimize the made-up loss function:\n\\[\nL(x, y, z) = (x + y)z\n\\]\nThe way a computer evaluates that function is by constructing a computational graph, consisting of several nodes and edges. It calculates the result by calculating the value at each node and passing it forward to the next node, until it reaches the end of the computation.\nBackpropagation uses this graph, storing intermediate computations that it will later need along the way. This is called the forward pass.\n\nLet’s name the nodes as follows:\n\\[\n\\begin{align}\n    q &= x + y \\\\\n    L &= zq\n\\end{align}\n\\]\nWe see that the derivatives are\n\\[\n\\frac{\\partial L}{\\partial z} = q,\n\\frac{\\partial L}{\\partial q} = z,\n\\frac{\\partial q}{\\partial x} = 1,\n\\frac{\\partial q}{\\partial y} = 1.\n\\]\nThese represent the effect that \\(z\\) and \\(q\\) have on \\(L\\), and the effect that \\(x\\) and \\(y\\) have on \\(q\\). However, we are not done yet! Remember that our goal was not to find these derivatives, but to find \\(\\frac{\\partial L}{\\partial x}, \\frac{\\partial L}{\\partial y}\\text{ } \\text{and } \\frac{\\partial L}{\\partial z}\\): the effect that \\(x\\), \\(y\\), and \\(z\\) have on the loss \\(L\\) at the end of the graph."
  },
  {
    "objectID": "posts/backpropagation/index.html#calculating-derivatives",
    "href": "posts/backpropagation/index.html#calculating-derivatives",
    "title": "Backpropagation: How Does Your Computer Calculate Gradients?",
    "section": "Calculating derivatives",
    "text": "Calculating derivatives\nLooking at the graph, we see that when the input to \\(q\\) changes by \\(\\partial x\\), its output changes by \\(\\frac{\\partial q}{\\partial x}\\) as a result. How much \\(L\\) changes in response to a change in \\(q\\) is given by \\(\\frac{\\partial L}{\\partial q}\\). So the effect that a change in \\(x\\) has on \\(L\\), is given by chaining these effects together: \\(\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial q}\\frac{\\partial q}{\\partial x}\\).\nFollowing the same reasoning, we see that \\(\\frac{\\partial L}{\\partial y} = \\frac{\\partial L}{\\partial q}\\frac{\\partial q}{\\partial y}\\).\nSince \\(z\\) directly affects \\(L\\) without intermediate nodes, the change in \\(L\\) from a change in \\(z\\) is simply given by \\(\\frac{\\partial L}{\\partial z}\\)\nWe compute these values by moving from the end of the graph back to the beginning, this is called the backward pass.\nDuring the forward pass, every node received input from its upstream nodes, performed a basic computation, and then passed the result forward to its downstream nodes. When the final node in the graph computes its output, the computation is done. At that point the backward pass will start.\n\nNote: A downstream node is one that comes after the flow of data (i.e., closer to the output), and an upstream node is one that comes before (i.e., closer to the input). In the backward pass, since data flows from output to input, the terms are used in reverse."
  },
  {
    "objectID": "posts/backpropagation/index.html#propagating-backward",
    "href": "posts/backpropagation/index.html#propagating-backward",
    "title": "Backpropagation: How Does Your Computer Calculate Gradients?",
    "section": "Propagating backward",
    "text": "Propagating backward\nLet’s zoom in on a random node during the backward pass.\n\n\n\nA node calculating how its inputs affect the loss, passing these values downstream.\n\n\nEventually, this node will receive a number from its output node, called the upstream gradient. This represents the change in the loss function all the way at the end of the graph when the output of this particular node changes. If the node receives multiple upstream gradients, it sums them up. After receiving the upstream gradient, the node computes local gradients, which represent how much each output of the node is affected by each input to the node. The node then calculates how each of its inputs affects the loss by multiplying each local gradient by the upstream gradient, and passes these downstream gradients to the respective input nodes. These input nodes then receive it as their own upstream gradient, repeating the process.\nThis entire process is a repeated application of the chain rule, which lets us compute how an input affects the final output through intermediate variables. When the process reaches the beginning of the graph, each input received an upstream gradient, meaning that we have the gradient of the loss function with respect to the inputs, and we are able to perform an iteration of gradient descent.\n\nTo summarize, a node:\n\nReceives an upstream gradient\nPasses along downstream gradients by multiplying the upstream gradient with local gradients.\n\nThe only thing left to do is to calculate these local gradients.\nWe do this by defining the local gradient for all node types we have in our graph beforehand.\nIn the case of an addition node, the local gradients are \\(1\\), so the downstream gradients are just the upstream gradient. For multiplication nodes, the local gradient with respect to an input node \\(a\\) is simply the product of all other input nodes. In a two-input case, this means that the gradient with respect to \\(a\\) is simply the other input. Other node types have simple gradient calculations as well!"
  },
  {
    "objectID": "posts/backpropagation/index.html#looking-back",
    "href": "posts/backpropagation/index.html#looking-back",
    "title": "Backpropagation: How Does Your Computer Calculate Gradients?",
    "section": "Looking back",
    "text": "Looking back\nThinking about calculating the gradient in terms of this computational graph has melted away all our initial problems. Remember that calculating the gradient by hand is tedious or infeasible, but using backpropagation it has become trivial! Secondly, we no longer need to recalculate the gradient when we change the structure of the model. This is because the building blocks of the graph remain the same, so whilst the graph could have a different structure, the backpropagation algorithm remains the same, so our modularity issue is solved! Finally, backpropagation does not need to repeat calculations, making it extremely fast, and suitable for training deep neural networks.\nThank you for reading! I recommend that you check out Justin Johnson’s excellent video, which goes in-depth on generalizing to vector and tensor valued functions, and which this post was based upon."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-3-transforms/index.html",
    "href": "posts/100-days-of-pytorch/day-3-transforms/index.html",
    "title": "Day 3: Pytorch Basics - Transforms",
    "section": "",
    "text": "Data comes in many different formats. On the other hand, PyTorch can only do machine learning with one data type, the tensor. Transforms can convert any data to a tensor. In this post, we will look at how to transform images. I will assume that you are familiar with PyTorch Datasets. If you are not, I recommend reading this post before you continue."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-3-transforms/index.html#why-do-we-need-transform",
    "href": "posts/100-days-of-pytorch/day-3-transforms/index.html#why-do-we-need-transform",
    "title": "Day 3: Pytorch Basics - Transforms",
    "section": "",
    "text": "Data comes in many different formats. On the other hand, PyTorch can only do machine learning with one data type, the tensor. Transforms can convert any data to a tensor. In this post, we will look at how to transform images. I will assume that you are familiar with PyTorch Datasets. If you are not, I recommend reading this post before you continue."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-3-transforms/index.html#how-do-pytorch-transforms-work",
    "href": "posts/100-days-of-pytorch/day-3-transforms/index.html#how-do-pytorch-transforms-work",
    "title": "Day 3: Pytorch Basics - Transforms",
    "section": "How do PyTorch transforms work?",
    "text": "How do PyTorch transforms work?\nAll built-in datasets from the torchvision module take the parameters transform and target_tranform. They take in a function that transforms input data into a tensor, following predefined steps. To avoid having to write these functions ourselves, the torchvision.transforms module come with an image-to-tensor transform, called ToTensor out of the box.\nLet’s see an example through the FashionMNIST dataset.\n\nimport torch\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision.transforms import ToTensor, Lambda\n\n\ndef our_own_transformation(target):\n    \"\"\"\n    Transformes target label to a one-hot tensor\n    example:\n\n    &gt;&gt;&gt; our_own_transformation(3)\n    &gt;&gt;&gt; torch.tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n    \"\"\"\n\n    zeros_list = torch.zeros(10, dtype=torch.float)\n    one_hot_index = torch.tensor(target)\n    one_hot_tensor = zeros_list.scatter_(0, one_hot_index, value=1)\n    return one_hot_tensor\n\nds_train = FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n    target_transform=Lambda(our_own_transformation)\n)\n\nds_test = FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n    target_transform=Lambda(our_own_transformation)\n)\n\nIn this code, we specified that we want to convert our training data to a tensor using the ToTensor method, and the target label to a tensor using our_own_transformation."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-3-transforms/index.html#further-reading",
    "href": "posts/100-days-of-pytorch/day-3-transforms/index.html#further-reading",
    "title": "Day 3: Pytorch Basics - Transforms",
    "section": "Further reading",
    "text": "Further reading\nThere are many more things we can do with transforms. We can rotate images, shift images, or we can chain transformations together to create a preprocessing pipeline. Since those usecases are too advanced for us at the moment, I will not cover them in this post. However, if you are curious or already more experienced, I recommend that you check out the example section on the Pytorch Website!"
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-4-building-and-training-a-neural-network/index.html",
    "href": "posts/100-days-of-pytorch/day-4-building-and-training-a-neural-network/index.html",
    "title": "Day 4 - PyTorch Basics: Building and Training a Neural Network",
    "section": "",
    "text": "Show imports\n\n\nimport os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n\nPyTorch allows us to create any neural network, using predefined building blocks from the module torch.nn. Every neural network we create should specify the forward method\n\nclass OurNeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Define neural net structure here, so we can store weights in them.\n        self.flatten = nn.Flatten()\n        self.linear_relu_chain = nn.Sequential(\n            nn.Linear(in_features=28*28, out_features=512),\n            nn.ReLU(),\n            nn.Linear(in_features=512, out_features=512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n    \n    def forward(self, input):\n        # Use neural net structure to pass input data through\n\n        input = self.flatten(input) # Shape: (28,28) -&gt; shape: (784)\n\n        predictions = self.linear_relu_chain(input) # Shape: (784) -&gt; shape: (512) -&gt; shape: (512) -&gt; shape: (10)\n        \n        return predictions\n\nLets instantiate it. In PyTorch, we also have to specify it on what device type we want to train our model. This allows for quicker training, depending on device type.\n\nmodel = OurNeuralNetwork().to(\"cpu\") # or cuda, mps, mtia, xpu\nprint(\"using cpu\")\n\nusing cpu\n\n\n\nmodel\n\nOurNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_chain): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)"
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-4-building-and-training-a-neural-network/index.html#defining-the-network",
    "href": "posts/100-days-of-pytorch/day-4-building-and-training-a-neural-network/index.html#defining-the-network",
    "title": "Day 4 - PyTorch Basics: Building and Training a Neural Network",
    "section": "",
    "text": "Show imports\n\n\nimport os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n\nPyTorch allows us to create any neural network, using predefined building blocks from the module torch.nn. Every neural network we create should specify the forward method\n\nclass OurNeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Define neural net structure here, so we can store weights in them.\n        self.flatten = nn.Flatten()\n        self.linear_relu_chain = nn.Sequential(\n            nn.Linear(in_features=28*28, out_features=512),\n            nn.ReLU(),\n            nn.Linear(in_features=512, out_features=512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n    \n    def forward(self, input):\n        # Use neural net structure to pass input data through\n\n        input = self.flatten(input) # Shape: (28,28) -&gt; shape: (784)\n\n        predictions = self.linear_relu_chain(input) # Shape: (784) -&gt; shape: (512) -&gt; shape: (512) -&gt; shape: (10)\n        \n        return predictions\n\nLets instantiate it. In PyTorch, we also have to specify it on what device type we want to train our model. This allows for quicker training, depending on device type.\n\nmodel = OurNeuralNetwork().to(\"cpu\") # or cuda, mps, mtia, xpu\nprint(\"using cpu\")\n\nusing cpu\n\n\n\nmodel\n\nOurNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_chain): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)"
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-4-building-and-training-a-neural-network/index.html#testing-the-network",
    "href": "posts/100-days-of-pytorch/day-4-building-and-training-a-neural-network/index.html#testing-the-network",
    "title": "Day 4 - PyTorch Basics: Building and Training a Neural Network",
    "section": "Testing the Network",
    "text": "Testing the Network\nLets simulate an 28x28 pixel image with some random numbers\n\nshape = (1, 28, 28)\nfake_image = torch.rand(shape, device=\"cpu\") # the tensor needs to be on the same device as the model\n\n\n\nShow plotting code\nimport matplotlib.pyplot as plt\n\nplt.imshow(fake_image.squeeze())\nplt.axis(\"off\")\nplt.title(\"Fake number - random 28x28 tensor\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nmodel_outputs = model(fake_image)\n\nEven though we specified nn.Linear(in_features=28*28), we set shape = (1, 28, 28) and not to (28, 28). This is because the model expects the first number to be the amount of images we use per batch of training. Since we are not actually training the model right now we set it to 1.\n\nprobabilities = nn.Softmax(dim=1)(model_outputs)\npredicted_label = probabilities.argmax(1)\npredicted_label.item()\n\n8\n\n\nOf course, the output is completely random, since the network was not trained and since the image was not actually a number."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-4-building-and-training-a-neural-network/index.html#training-and-evaluating-the-network",
    "href": "posts/100-days-of-pytorch/day-4-building-and-training-a-neural-network/index.html#training-and-evaluating-the-network",
    "title": "Day 4 - PyTorch Basics: Building and Training a Neural Network",
    "section": "Training and Evaluating the Network",
    "text": "Training and Evaluating the Network\nLet’s train the network on the FashionMNIST dataset to classify images. For this we import the dataset using the code from a previous post explaining PyTorch Datasets and DataLoaders\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=transforms.ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=transforms.ToTensor()\n)\n\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n\nWe need to specify 3 parameters:\n\nNumber of Epochs - How many times to iterate over the entire dataset.\nLearning Rate - A scaling factor specifying how much to update the model parameters each iteration\nBatch Size - How much examples to iterate over before updating parameters.\n\nPer batch of images, model parameters are updated, until all images in the dataset have been seen. This process is repeated for the specified number of epochs.\n\nlearning_rate = 1e-3\nbatch_size = 64\nepochs = 5\n\nIn each epoch we also want to check the model performance (to see if it is improving). That is why we will build two loops.\n\nTraining Loop - Update parameters by showing model images with labels (from train_dataloader)\nChecking Loop - Evaluate model performance with learned parameters from the training loop on new images (from test_dataloader)\n\n\ndef train_loop(dataloader, model, loss_func, optimizer):\n    size = len(dataloader.dataset)\n    model.train() # Set model to training mode\n\n    # Update parameters each new batch\n    for batch, (images, labels) in enumerate(dataloader):\n        model_predictions = model(images)\n        loss = loss_func(model_predictions, labels)\n\n        # Compute gradient with backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        # Something to look at while model trains\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * batch_size + len(images)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n\ndef checking_loop(dataloader, model, loss_func):\n    size = len(dataloader.dataset)\n    number_of_batches = len(dataloader)\n    test_loss, correct_amount = 0, 0\n\n    model.eval() # Set model to check/test mode\n\n    with torch.no_grad(): # We don't need to update parameters anymore. This speeds up testing.\n\n        # This dataloader contains the test images\n        for images, labels in dataloader:\n            model_predictions = model(images)\n            \n            loss = loss_func(model_predictions, labels).item()\n            test_loss += loss\n\n            predicted_labels = nn.Softmax(dim=1)(model_predictions).argmax(1)\n            correct = predicted_labels == labels\n            # Turn every 'True' into a 1, and sum over them, converting the resulting tensor to a python integer\n            correct_amount += correct.type(torch.float).sum().item()\n\n    test_loss /= number_of_batches\n    correct_amount /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct_amount):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")\n\nLets train our model, passing a loss function and optimizer.\n\nloss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n\n# Train the model\nepochs = 10\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train_dataloader, model, loss_function, optimizer)\n    checking_loop(test_dataloader, model, loss_function)\nprint(\"Done!\")\n\n\n\nShow output\n\n\n\n\nEpoch 1\n-------------------------------\nloss: 1.469673  [   64/60000]\nloss: 1.433510  [ 6464/60000]\nloss: 1.416146  [12864/60000]\nloss: 1.377274  [19264/60000]\nloss: 1.353796  [25664/60000]\nloss: 1.429164  [32064/60000]\nloss: 1.270087  [38464/60000]\nloss: 1.251621  [44864/60000]\nloss: 1.361366  [51264/60000]\nloss: 1.273360  [57664/60000]\nTest Error: \n Accuracy: 63.7%, Avg loss: 1.244470 \n\nEpoch 2\n-------------------------------\nloss: 1.250782  [   64/60000]\nloss: 1.358424  [ 6464/60000]\nloss: 1.121522  [12864/60000]\nloss: 1.036822  [19264/60000]\nloss: 1.188575  [25664/60000]\nloss: 1.127103  [32064/60000]\nloss: 1.138114  [38464/60000]\nloss: 1.059073  [44864/60000]\nloss: 1.033557  [51264/60000]\nloss: 1.098557  [57664/60000]\nTest Error: \n Accuracy: 65.0%, Avg loss: 1.083130 \n\nEpoch 3\n-------------------------------\nloss: 1.070835  [   64/60000]\nloss: 1.022191  [ 6464/60000]\nloss: 1.005594  [12864/60000]\nloss: 0.993012  [19264/60000]\nloss: 1.047417  [25664/60000]\nloss: 1.001495  [32064/60000]\nloss: 1.095251  [38464/60000]\nloss: 0.926997  [44864/60000]\nloss: 0.960782  [51264/60000]\nloss: 0.937367  [57664/60000]\nTest Error: \n Accuracy: 66.1%, Avg loss: 0.979457 \n\nEpoch 4\n-------------------------------\nloss: 0.900051  [   64/60000]\nloss: 1.099103  [ 6464/60000]\nloss: 1.052053  [12864/60000]\nloss: 0.843110  [19264/60000]\nloss: 0.914962  [25664/60000]\nloss: 1.017330  [32064/60000]\nloss: 0.707650  [38464/60000]\nloss: 0.890666  [44864/60000]\nloss: 1.078490  [51264/60000]\nloss: 0.758047  [57664/60000]\nTest Error: \n Accuracy: 67.3%, Avg loss: 0.909492 \n\nEpoch 5\n-------------------------------\nloss: 0.935071  [   64/60000]\nloss: 0.930360  [ 6464/60000]\nloss: 0.886458  [12864/60000]\nloss: 0.747989  [19264/60000]\nloss: 0.919060  [25664/60000]\nloss: 0.857149  [32064/60000]\nloss: 0.808115  [38464/60000]\nloss: 0.957309  [44864/60000]\nloss: 0.915866  [51264/60000]\nloss: 1.035016  [57664/60000]\nTest Error: \n Accuracy: 68.0%, Avg loss: 0.857042 \n\nEpoch 6\n-------------------------------\nloss: 0.711309  [   64/60000]\nloss: 0.731404  [ 6464/60000]\nloss: 0.778495  [12864/60000]\nloss: 0.826608  [19264/60000]\nloss: 0.690381  [25664/60000]\nloss: 0.793883  [32064/60000]\nloss: 1.049005  [38464/60000]\nloss: 0.860935  [44864/60000]\nloss: 0.850578  [51264/60000]\nloss: 0.894870  [57664/60000]\nTest Error: \n Accuracy: 69.2%, Avg loss: 0.820537 \n\nEpoch 7\n-------------------------------\nloss: 0.751307  [   64/60000]\nloss: 0.690765  [ 6464/60000]\nloss: 0.885832  [12864/60000]\nloss: 0.810388  [19264/60000]\nloss: 0.656271  [25664/60000]\nloss: 0.795354  [32064/60000]\nloss: 0.873639  [38464/60000]\nloss: 0.952544  [44864/60000]\nloss: 0.621379  [51264/60000]\nloss: 0.782824  [57664/60000]\nTest Error: \n Accuracy: 70.0%, Avg loss: 0.791091 \n\nEpoch 8\n-------------------------------\nloss: 0.706885  [   64/60000]\nloss: 0.791194  [ 6464/60000]\nloss: 0.665691  [12864/60000]\nloss: 0.586563  [19264/60000]\nloss: 0.746921  [25664/60000]\nloss: 0.670890  [32064/60000]\nloss: 0.818113  [38464/60000]\nloss: 0.725863  [44864/60000]\nloss: 0.793836  [51264/60000]\nloss: 0.689501  [57664/60000]\nTest Error: \n Accuracy: 71.0%, Avg loss: 0.764419 \n\nEpoch 9\n-------------------------------\nloss: 0.579552  [   64/60000]\nloss: 0.783948  [ 6464/60000]\nloss: 0.766569  [12864/60000]\nloss: 0.831361  [19264/60000]\nloss: 0.964704  [25664/60000]\nloss: 0.772870  [32064/60000]\nloss: 0.836838  [38464/60000]\nloss: 0.806005  [44864/60000]\nloss: 0.795276  [51264/60000]\nloss: 0.934505  [57664/60000]\nTest Error: \n Accuracy: 72.8%, Avg loss: 0.744880 \n\nEpoch 10\n-------------------------------\nloss: 0.493929  [   64/60000]\nloss: 0.790016  [ 6464/60000]\nloss: 0.750857  [12864/60000]\nloss: 0.762535  [19264/60000]\nloss: 0.822756  [25664/60000]\nloss: 0.723158  [32064/60000]\nloss: 0.535035  [38464/60000]\nloss: 0.708430  [44864/60000]\nloss: 0.695287  [51264/60000]\nloss: 0.616080  [57664/60000]\nTest Error: \n Accuracy: 73.5%, Avg loss: 0.724422 \n\nDone!"
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-4-building-and-training-a-neural-network/index.html#further-reading",
    "href": "posts/100-days-of-pytorch/day-4-building-and-training-a-neural-network/index.html#further-reading",
    "title": "Day 4 - PyTorch Basics: Building and Training a Neural Network",
    "section": "Further reading",
    "text": "Further reading\n\ntorch.nn under the hood\ntorch.nn examples"
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html",
    "href": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html",
    "title": "Day 2: PyTorch Basics - Dataset and DataLoader",
    "section": "",
    "text": "According to PyTorch:\n“Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity.”\nIn short, we try to prevent messy notebooks and data leakage by seperating our data and data processing from our model. This is done with the Dataset and DataLoader classes."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#why-seperate-classes-anyway",
    "href": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#why-seperate-classes-anyway",
    "title": "Day 2: PyTorch Basics - Dataset and DataLoader",
    "section": "",
    "text": "According to PyTorch:\n“Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity.”\nIn short, we try to prevent messy notebooks and data leakage by seperating our data and data processing from our model. This is done with the Dataset and DataLoader classes."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#how-to-use-the-pytorch-dataset-class",
    "href": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#how-to-use-the-pytorch-dataset-class",
    "title": "Day 2: PyTorch Basics - Dataset and DataLoader",
    "section": "How to use the PyTorch Dataset class?",
    "text": "How to use the PyTorch Dataset class?\nWe will see that working with these classes is pretty easy. PyTorch has commonly datasets built-in, ready to work with. Since the fashionMNIST dataset is used for computer vision-related tasks, it is stored in the torchvision module.\nThe fashionMNIST dataset takes 4 parameters:\n\nroot is the folder name where the data will be stored in.\ntrain specifies if you want to download the training or testing data.\ndownload downloads the data from the internet when you don’t have it locally yet.\ntransform takes a PIL image and transforms it to a tensor.\n\nLet’s import the fashionMNIST dataset.\n\n\nShow imports\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\n\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)"
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#dataset-attributes",
    "href": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#dataset-attributes",
    "title": "Day 2: PyTorch Basics - Dataset and DataLoader",
    "section": "Dataset attributes",
    "text": "Dataset attributes\nNow that we saved the fashionMNIST dataset in a Dataset object, what can we do with it?\n\n# Dataset summary \ntraining_data\n\n# Class labels\ntraining_data.classes\n\n# Select a row of data\nimg, label = training_data[0]\n\n\n\nShow output\n\n\n\ntraining_data:\nDataset FashionMNIST\n    Number of datapoints: 60000\n    Root location: data\n    Split: Train\n    StandardTransform\nTransform: ToTensor() \n\ntraining_data.classes:\n[\n  'T-shirt/top',\n  'Trouser',\n  'Pullover',\n  'Dress',\n  'Coat',\n  'Sandal',\n  'Shirt',\n  'Sneaker',\n  'Bag',\n  'Ankle boot',\n] \n\nlabel:\n9\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt’s always a good idea to inspect your data. Let’s look at img and label from the first row of data.\n\n\n\nplt.imshow(img.squeeze(), cmap='gray')\nplt.set_title(training_data.classes[label])\n\n\n\n\n\n\n\n\n\n\nThat indeed looks like an ankle boot, very nice!"
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#using-your-own-data-with-dataset",
    "href": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#using-your-own-data-with-dataset",
    "title": "Day 2: PyTorch Basics - Dataset and DataLoader",
    "section": "Using your own data with Dataset",
    "text": "Using your own data with Dataset\nWe don’t always want to use predefined datasets. Very often we have our own data that we want to use. Pytorch has two ways of creating your own dataset. the map-style dataset, which is most commonly used, and the iterable-style dataset, for data that comes in on the fly, such as user-log data. The map-style behaves as you would likely expect from a dataset: you know its length beforehand, and you can select data through an index. For this, map-style datasets need to implement the __len__ and __getitem__ method. Let’s use our own data with a map-style Dataset.\nConsider the case where we have a csv file of image file names and the labels associated with them.\n\n\n\n\n\n\n\n\n\nItem Name\nLabel\n\n\n\n\n0\ntshirt1.jpg\n0\n\n\n1\ntshirt2.jpg\n0\n\n\n2\n...\n...\n\n\n3\nankleboot999.jpg\n9\n\n\n\n\n\n\n\nWe would define a custom dataset class as such:\n\nimport os\nimport pandas as pd\nfrom torchvision.io import read_image\n\nclass CustomImageDataSet(Dataset):\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform # optional\n        self.target_transform = target_transform # optional\n    \n    def __len__(self):\n        return len(self.img_labels)\n    \n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n\n        img = read_image(img_path)\n        label = self.img_labels.iloc[idx, 1]\n\n        if self.transform:\n            img = self.transform(img)\n        if self.target_transform:\n            label = self.target_transform(label)\n        return img, label\n\nThe code above is a bit involved, so let’s walk through it.\n\n__init__ stores values that we pass in variables, and reads the labels file annotations_file.\n__len__ specifies the size of the dataset by returning the amount of labels.\n\n__getitem__ creates a path to an image. For example, if img_dir='images' and idx=0, then img_path is images/tshirt1.jpg. It then reads the image using a predefined PyTorch function, and reads the label. If any transformations are specified they are applied.\nWe can now select images and labels from our dataset, much like we did earlier in @selecting-img-label"
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#how-to-use-the-pytorch-dataloader-class",
    "href": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#how-to-use-the-pytorch-dataloader-class",
    "title": "Day 2: PyTorch Basics - Dataset and DataLoader",
    "section": "How to use the PyTorch DataLoader class",
    "text": "How to use the PyTorch DataLoader class\nIn the code above we only specified how to return a single (image, label) pair. In practice, we typically use lots of images and labels (called batches) for per training step. Additionally, we want to shuffle data (to prevent overfitting) and we want to speed up the process using multiprocessing. This is where the DataLoader class steps in. Let’s use the DataLoader to retrieve 64 images at once.\n\nfrom torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n\n# Get data for one simulated training step\ntrain_image_features, train_labels = next(iter(train_dataloader))\n\nRemember that these images are the data used for one training step. Let’s see the batch of images that our dataloader just sent us.\n\n\nShow visualization code\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 8, 8\n\nfor i in range(cols * rows):\n    idx = i\n    img = train_image_features[idx]\n\n    figure.add_subplot(rows, cols, i +1)\n    plt.imshow(img.squeeze(), cmap=\"gray\")\n    plt.title(training_data.classes[train_labels[idx]], pad=1, fontsize=10)\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThat’s a lot of images! By using DataLoader, we have an easy way to retrieve lots of images from our data at once."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#different-ways-to-shuffle",
    "href": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#different-ways-to-shuffle",
    "title": "Day 2: PyTorch Basics - Dataset and DataLoader",
    "section": "Different ways to shuffle",
    "text": "Different ways to shuffle\nWe can also control how data is shuffled (or in other words, the way that the random batch is sampled). We do this by setting the sampler parameter of the DataLoader. When doing this we have to set shuffle=False, since the shuffle parameter essentially sets the sampler parameter for us.\n\nimport numpy as np\nfrom torch.utils.data.sampler import SequentialSampler, SubsetRandomSampler\n\n# Returns images in order.\n# The first batch will have the first 32 images, the second batch will have image 33-64, etc.\ntrain_loader = DataLoader(training_data, batch_size=32, sampler=SequentialSampler(training_data))\n\n# Sample randomly, only including the images 50-100.\nindices = np.arange(50, 101)\ntrain_loader = DataLoader(training_data, batch_size=32, sampler=SubsetRandomSampler(indices))\n\nThese DataLoaders will select images in order of appearance in data, and randomly sample a subset of the data. Read about all the ways to sample in the PyTorch documentation.\nTo summarize: With Dataset and DataLoader, PyTorch makes it easy to manage data efficiently and flexibly. In future posts, we’ll explore how these tools integrate into full training loops."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#further-reading",
    "href": "posts/100-days-of-pytorch/day-2-datasets-dataloaders/index.html#further-reading",
    "title": "Day 2: PyTorch Basics - Dataset and DataLoader",
    "section": "Further Reading",
    "text": "Further Reading\nPyTorch also offers ways to speed up sampling through multiprocessing and memory-pinning, which are both reasonably complicated and have some warnings attached to them, the latter being the reason that I did not include them in this post. If you are interested, or already know all about multiprocessing and GPU computations you can read about the topics here. Thanks for reading all the way to the end, I hope to see you on day 3!"
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-1-tensors/index.html",
    "href": "posts/100-days-of-pytorch/day-1-tensors/index.html",
    "title": "Day 1: Pytorch Basics - Tensors",
    "section": "",
    "text": "If you are familiar with math, it might help to think about a tensor as an n-dimensional matrix. If you are not familiar with math, or just want a better explanation, you can think of a tensor as a collection of structured numbers that we can do quick math with. A tensor has two properties: shape, and dimension. Shape means how many numbers the tensor has along each axis. Dimension means the amount of axes that the tensor has. In the picture below, the dimension corresponds with the number of colored arrows, and the shape is denoted below the tensor.\n\n\n\nA vector (1D), a matrix (2D) and a 3 dimensional tensor (3D) are all tensors."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-1-tensors/index.html#what-is-a-tensor",
    "href": "posts/100-days-of-pytorch/day-1-tensors/index.html#what-is-a-tensor",
    "title": "Day 1: Pytorch Basics - Tensors",
    "section": "",
    "text": "If you are familiar with math, it might help to think about a tensor as an n-dimensional matrix. If you are not familiar with math, or just want a better explanation, you can think of a tensor as a collection of structured numbers that we can do quick math with. A tensor has two properties: shape, and dimension. Shape means how many numbers the tensor has along each axis. Dimension means the amount of axes that the tensor has. In the picture below, the dimension corresponds with the number of colored arrows, and the shape is denoted below the tensor.\n\n\n\nA vector (1D), a matrix (2D) and a 3 dimensional tensor (3D) are all tensors."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-1-tensors/index.html#how-to-create-a-tensor",
    "href": "posts/100-days-of-pytorch/day-1-tensors/index.html#how-to-create-a-tensor",
    "title": "Day 1: Pytorch Basics - Tensors",
    "section": "How to create a tensor?",
    "text": "How to create a tensor?\nFirst, import torch. After that we can create a tensor in two ways. From existing data, or with new data.\n\n# Create a tensor from existing data\ndata = [[1, 2], [3, 4]]\ntensor_from_data = torch.tensor(data)\nprint(tensor_from_data)\n\n# Create a tensor with new data\nones_tensor = torch.ones((2,2))\nprint(ones_tensor)\n\n\n\nShow output\n\n\n\ntensor([[1, 2],\n        [3, 4]])\ntensor([[1., 1.],\n        [1., 1.]])"
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-1-tensors/index.html#how-to-do-math-with-tensors",
    "href": "posts/100-days-of-pytorch/day-1-tensors/index.html#how-to-do-math-with-tensors",
    "title": "Day 1: Pytorch Basics - Tensors",
    "section": "How to do math with tensors?",
    "text": "How to do math with tensors?\nThere are three ways to perform a math operation in PyTorch. Lets see an example with addition.\n\n\nInitialization code\n# Initialize tensors to do math with\nshape = (2,2)\ntensor1 = torch.rand(shape)\ntensor2 = torch.ones(shape)\n\n\n\n# 1. Python operators\ntensor1 + tensor2\n\n# 2. Built-in tensor method\ntensor1.add(tensor2)\n\n# 3. Output tensor\noutput_tensor = torch.zeros(shape)\ntorch.add(tensor1, tensor2, out=output_tensor)\n\n\n\n\n\n\n\nNote\n\n\n\nCurrently, I am not sure about the difference between these three methods. I imagine that in a situation where we need to choose between these, one of the three method will feel most natural to use."
  },
  {
    "objectID": "posts/100-days-of-pytorch/day-1-tensors/index.html#some-built-in-tensor-methods",
    "href": "posts/100-days-of-pytorch/day-1-tensors/index.html#some-built-in-tensor-methods",
    "title": "Day 1: Pytorch Basics - Tensors",
    "section": "Some built-in tensor methods",
    "text": "Some built-in tensor methods\nThere are over 1200 methods that we can perform on tensors. They can all be found at the PyTorch documentation. I skimmed through them and will give my beginner-opinion on which ones I believe will allow us to get a good start.\n\n\nMutating and indexing tensors\n\nKnowing how to select and change individual elements as well as groups of elements is an essential skill to have, and easily learned. Indexing tensors works a lot like python list and numpy array indexing.\n\ntensor = torch.rand((3, 3))\n\nfirst_row = tensor[0]\nfirst_two_row = tensor[:2]\nfirst_col = tensor[:, 0]\nfirst_two_col = tensor[:, :2]\n\n\n\nShow output\n\n\n\nEntire tensor:\ntensor([[0.4917, 0.5003, 0.4915],\n        [0.6349, 0.1063, 0.3295],\n        [0.0865, 0.7065, 0.6171]]) \n\nfirst row:\ntensor([0.4917, 0.5003, 0.4915]) \n\nfirst two rows:\ntensor([[0.4917, 0.5003, 0.4915],\n        [0.6349, 0.1063, 0.3295]]) \n\nfirst column:\ntensor([0.4917, 0.6349, 0.0865]) \n\nfirst two columns:\ntensor([[0.4917, 0.5003],\n        [0.6349, 0.1063],\n        [0.0865, 0.7065]])\n\n\n\nLets now look at how to combine multiple tensors.\n\ntensor_ones = torch.ones((3,3))\ntensor_ones[:, 0] = 4\n\n# Combine multiple tensors horizontally\nwide_combine = torch.cat((tensor, tensor_ones), dim=1)\neven_wider_combine = torch.cat((tensor, tensor_ones, tensor_ones), dim=1)\n\n# Combine multiple tensors vertically\nhigh_combine = torch.cat((tensor, tensor_ones), dim=0)\n\n\n\nShow output\n\n\n\nHorizontal combine:\ntensor([[0.4917, 0.5003, 0.4915, 4.0000, 1.0000, 1.0000],\n        [0.6349, 0.1063, 0.3295, 4.0000, 1.0000, 1.0000],\n        [0.0865, 0.7065, 0.6171, 4.0000, 1.0000, 1.0000]]) \n\nWe can combine any number of tensors we want:\ntensor([[0.4917, 0.5003, 0.4915, 4.0000, 1.0000, 1.0000, 4.0000, 1.0000, 1.0000],\n        [0.6349, 0.1063, 0.3295, 4.0000, 1.0000, 1.0000, 4.0000, 1.0000, 1.0000],\n        [0.0865, 0.7065, 0.6171, 4.0000, 1.0000, 1.0000, 4.0000, 1.0000, 1.0000]]) \n\nVertical combine\ntensor([[0.4917, 0.5003, 0.4915],\n        [0.6349, 0.1063, 0.3295],\n        [0.0865, 0.7065, 0.6171],\n        [4.0000, 1.0000, 1.0000],\n        [4.0000, 1.0000, 1.0000],\n        [4.0000, 1.0000, 1.0000]])\n\n\n\n\n\n\nComparing tensors\n\nJust like with the math operations, we have different notation options to choose from when comparing tensors.\n\ntensor1 = torch.zeros((2,2))\ntensor1[0][0] = 10\ntensor2 = torch.ones((2,2))\n\n# Check if two tensors are equal\ntorch.equal(tensor1, tensor2)\ntensor1 == tensor2\n\n# Check if one tensor is greater or equal to another tensor\ntorch.greater_equal(tensor1, tensor2)\ntorch.ge(tensor1, tensor2)\ntensor1 &gt;= tensor2\n\n\n\nShow output\n\n\n\nTensor 1 equals tensor 2:\ntensor([[False, False],\n        [False, False]]) \n\nTensor 1 &gt;= tensor 2:\ntensor([[ True, False],\n        [False, False]])\n\n\n\nOther comparison operators are implemented like the ones shown above in the way you probably expect. If you can’t find the one you’re looking for, there exists a list of all comparison operators on the PyTorch website.\n\n\n\nMore creation methods\n\nBeing able to instantiate a tensor with other values than ones and zeros is also possible.\n\n# Create a tensor filled with the number 3\nthrees = torch.full(size=(2,2), fill_value=3)\n\n# Create a tensor based on the shape of another\ntensor_with_shape = torch.rand((4, 3))\ntensor_zeros = torch.zeros_like(tensor_with_shape)\n\n\n\nShow output\n\n\n\nthrees:\ntensor([[3, 3],\n        [3, 3]]) \n\ntensor_with_shape:\ntensor([[0.9459, 0.3627, 0.1254],\n        [0.3131, 0.3433, 0.3845],\n        [0.2971, 0.2271, 0.8955],\n        [0.5679, 0.0303, 0.2326]]) \n\ntensor_zeros:\ntensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe way we created tensor_zeros based on the shape of another tensor using zeros_like can also be done to the other ways we learned how to create a tensor, just by suffixing the method with _like.\n\n\n\nRandom numbers\n\nshape = (3,3)\n# Set an optional seed for reproducibility\ntorch.manual_seed(1)\n\n# Random integers \nrand_int = torch.randint(10, shape)\nrand_int_lower_bound = torch.randint(8, 10, shape)\n\n# Tensor values drawn from a distribution\nnormal = torch.randn(shape)\n\nprobability_tensor = torch.empty(shape).uniform_(0, 1) # The '_' suffix modifies the variable in place\nbernoulli =torch.bernoulli(probability_tensor) # Pass a tensor with values of how likely a '1' is.\n\n# Shuffle numbers from 0 to n-1 \npermutation = torch.randperm(10)\n\n\n\nShow output\n\n\n\nrand_int):\ntensor([[5, 9, 4],\n        [8, 3, 3],\n        [1, 1, 9]]) \n\nrand_int_lower_bound:\ntensor([[8, 8, 9],\n        [8, 9, 9],\n        [8, 8, 9]]) \n\nnormal:\ntensor([[-1.1948,  0.0250, -0.7627],\n        [ 1.3969, -0.3245,  0.2879],\n        [ 1.0579,  0.9621,  0.3935]]) \n\nprobability_tensor: \n\ntensor([[0.7140, 0.2676, 0.9906],\n        [0.2885, 0.8750, 0.5059],\n        [0.2366, 0.7570, 0.2346]])\nbernoulli:\ntensor([[1., 0., 1.],\n        [1., 1., 0.],\n        [0., 0., 0.]]) \n\npermutation:\ntensor([1, 9, 3, 6, 8, 0, 5, 2, 7, 4]) \n\n\n\n\n\nThank you for reading! Note that these are just the basics. Now is your time to do some work yourself. Read the documentation, and try some basic operations. As a starter, this post did not cover the different linear algebra operations, even though they are very useful! You might also peek at the more technical GPU operations. See you on day 2!"
  },
  {
    "objectID": "100-days-of-pytorch.html",
    "href": "100-days-of-pytorch.html",
    "title": "100 Days Of PyTorch",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nDay 4 - PyTorch Basics: Building and Training a Neural Network\n\n\nLearn how to use torch.nn to build the neural network structure, and how to train the model to recognize images, using a training and evaluation loop. It assumes you are already familiar with the theory behind neural networks (i.e loss functions, gradient descent).\n\n\n\n\n\nMay 12, 2025\n\n\nLiam Groen\n\n\n\n\n\n\n\n\n\n\n\n\nDay 3: Pytorch Basics - Transforms\n\n\n\n\n\n\n\n\nMay 10, 2025\n\n\nLiam Groen\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2: PyTorch Basics - Dataset and DataLoader\n\n\nIn this post we learn about how to download datasets with PyTorch Datasets, and how to retrieve data from them for training an ML model while keeping data and model separate. We will see that working with these classes is pretty easy, and allows us to use all kinds of handy built-in methods.\n\n\n\n\n\nMay 9, 2025\n\n\nLiam Groen\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1: Pytorch Basics - Tensors\n\n\nTensors lie at the heart of PyTorch. If we want to be proficient in PyTorch, we need to know what we can do with the Tensor object. There are over 1200 possible operations to perform on tensors, but only a few are needed to get started with PyTorch. In this post we learn what a Tensor is and how to perform basic operations with them. Familiarity with python programming is assumed.\n\n\n\n\n\nMay 8, 2025\n\n\nLiam Groen\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Hi, I’m Liam, a student on a journey to understand the world through machine learning, coding, data science, and mathematics.\nThis blog is my digital lab notebook — a place where I explain what I’m learning, work through complex topics, and share insights that might help others learning too.\nWhether you’re just starting out or deep into the field, I hope you find something useful here.\n\n\nWriting is thinking. By distilling ideas into blog posts, I’m aiming to:\n\nDeepen my understanding of technical topics.\nBuild a consistent habit of learning and reflection.\nCreate a public portfolio of work as I grow in this field.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\nLearn how to use torch.nn to build the neural network structure, and how to train the model to recognize images, using a training and evaluation loop. It assumes you are…\n\n\n\nLiam Groen\n\n\nMay 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLiam Groen\n\n\nMay 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIn this post we learn about how to download datasets with PyTorch Datasets, and how to retrieve data from them for training an ML model while keeping data and model…\n\n\n\nLiam Groen\n\n\nMay 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensors lie at the heart of PyTorch. If we want to be proficient in PyTorch, we need to know what we can do with the Tensor object. There are over 1200 possible operations…\n\n\n\nLiam Groen\n\n\nMay 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow backpropagation is used to compute gradients in neural networks, using computational graphs and the chain rule.\n\n\n\nLiam Groen\n\n\nApr 20, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#why-i-blog",
    "href": "index.html#why-i-blog",
    "title": "Home",
    "section": "",
    "text": "Writing is thinking. By distilling ideas into blog posts, I’m aiming to:\n\nDeepen my understanding of technical topics.\nBuild a consistent habit of learning and reflection.\nCreate a public portfolio of work as I grow in this field."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Home",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\nLearn how to use torch.nn to build the neural network structure, and how to train the model to recognize images, using a training and evaluation loop. It assumes you are…\n\n\n\nLiam Groen\n\n\nMay 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLiam Groen\n\n\nMay 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIn this post we learn about how to download datasets with PyTorch Datasets, and how to retrieve data from them for training an ML model while keeping data and model…\n\n\n\nLiam Groen\n\n\nMay 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensors lie at the heart of PyTorch. If we want to be proficient in PyTorch, we need to know what we can do with the Tensor object. There are over 1200 possible operations…\n\n\n\nLiam Groen\n\n\nMay 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow backpropagation is used to compute gradients in neural networks, using computational graphs and the chain rule.\n\n\n\nLiam Groen\n\n\nApr 20, 2025\n\n\n\n\n\n\nNo matching items"
  }
]